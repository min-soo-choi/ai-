# sheet_review.py
# -*- coding: utf-8 -*-
import json
import time
import re
from typing import Dict, Any, List

import streamlit as st
import pandas as pd
import gspread
import google.generativeai as genai
from google.oauth2.service_account import Credentials

# ---------------------------------------------------
# 1. Gemini / Google Sheets í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# ---------------------------------------------------

# Gemini í‚¤ (secretsì—ì„œ ì½ê¸°)
API_KEY = st.secrets.get("GEMINI_API_KEY")
if not API_KEY:
    st.error("GEMINI_API_KEYê°€ secretsì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

genai.configure(api_key=API_KEY)
MODEL_ID = "gemini-2.0-flash-001"
model = genai.GenerativeModel(MODEL_ID)

# ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ (JSON ì „ì²´ë¥¼ secretsì— ë„£ì–´ë‘ )
raw = st.secrets["GCP_SERVICE_ACCOUNT_JSON"]

if isinstance(raw, dict):
    service_info = dict(raw)
elif isinstance(raw, str):
    service_info = json.loads(raw)
else:
    st.error("GCP_SERVICE_ACCOUNT_JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]

creds = Credentials.from_service_account_info(service_info, scopes=SCOPES)
gs_client = gspread.authorize(creds)


# ---------------------------------------------------
# 2. ì‹œíŠ¸ ì»¬ëŸ¼ ì´ë¦„ (ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ)
# ---------------------------------------------------

STATUS_COL = "STATUS"
ORIGINAL_TEXT_COL = "content"                   # ì˜ì–´ ì›ë¬¸
ORIGINAL_MD_COL = "content_markdown"           # ì˜ì–´ ë§ˆí¬ë‹¤ìš´
TRANSLATION_TEXT_COL = "content_translated"    # í•œêµ­ì–´ ë²ˆì—­
TRANSLATION_MD_COL = "content_markdown_translated"  # í•œêµ­ì–´ ë§ˆí¬ë‹¤ìš´

SUSPICION_SCORE_COL = "SCORE"
CONTENT_TYPO_REPORT_COL = "CONTENT_TYPO_REPORT"      # ì˜ì–´ ê²€ìˆ˜ ê²°ê³¼ (plain)
TRANSLATED_COL = "TRANSLATED_TYPO_REPORT"            # í•œêµ­ì–´ ê²€ìˆ˜ ê²°ê³¼ (plain)
MARKDOWN_REPORT_COL = "MARKDOWN_REPORT"              # ë§ˆí¬ë‹¤ìš´ ê´€ë ¨ ì˜¤ë¥˜ (en/ko í†µí•©)


# ---------------------------------------------------
# 3. ê³µí†µ ìœ í‹¸: ë¬¸ì/ì–¸ì–´ íŒë³„
# ---------------------------------------------------

def contains_hangul(text: str) -> bool:
    return any('ê°€' <= ch <= 'í£' for ch in text)


def contains_latin(text: str) -> bool:
    return any(('a' <= ch <= 'z') or ('A' <= ch <= 'Z') for ch in text)


# ---------------------------------------------------
# 4. ê³µí†µ ìœ í‹¸: ë¦¬í¬íŠ¸ í›„ì²˜ë¦¬ / ë¬¸ì¥ë¶€í˜¸ ê°•ì œ / hallucination í•„í„°
# ---------------------------------------------------

def dedup_korean_bullet_lines(report: str) -> str:
    """
    í•œêµ­ì–´ bullet ë¦¬í¬íŠ¸ì—ì„œ ì˜ë¯¸ê°€ ê²¹ì¹˜ëŠ” ì¤„ì„ ì •ë¦¬í•œë‹¤.
    - ì™„ì „íˆ ë™ì¼í•œ ì¤„ì€ í•˜ë‚˜ë§Œ ë‚¨ê¹€
    - 'ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ'ë¥˜ì—ì„œ ì›ë¬¸ì´ ë¶€ë¶„ ë¬¸ìì—´ ê´€ê³„ì´ë©´ ë” ê¸´ ìª½ë§Œ ìœ ì§€
      (ì˜ˆ: 'í–ˆê³ .' vs 'ì—­í• ì„ í–ˆê³ .' -> í›„ìë§Œ ë‚¨ê¹€)
    """
    if not report:
        return ""

    lines = [l.strip() for l in report.splitlines() if l.strip()]
    if not lines:
        return ""

    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':\s*(.+)$", re.UNICODE)

    # 1ì°¨: ì™„ì „ ì¤‘ë³µ ì œê±°
    unique_lines = []
    seen = set()
    for l in lines:
        if l not in seen:
            unique_lines.append(l)
            seen.add(l)

    # 2ì°¨: ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ ê´€ë ¨ ì¤‘ë³µ ì œê±°
    entries = []
    for idx, l in enumerate(unique_lines):
        m = pattern.match(l)
        if not m:
            entries.append({"idx": idx, "raw": l, "orig": None, "fixed": None, "msg": ""})
            continue
        orig, fixed, msg = m.group(1), m.group(2), m.group(3)
        entries.append({"idx": idx, "raw": l, "orig": orig, "fixed": fixed, "msg": msg})

    to_drop = set()
    for i, e1 in enumerate(entries):
        if not e1["orig"] or "ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ" not in e1["msg"]:
            continue
        for j, e2 in enumerate(entries):
            if i == j or not e2["orig"] or "ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ" not in e2["msg"]:
                continue

            o1, o2 = e1["orig"], e2["orig"]
            # ë” ì§§ì€ ê²ƒì´ ë” ê¸´ ê²ƒì˜ ë¶€ë¶„ ë¬¸ìì—´ì´ë©´ ì§§ì€ ê²ƒ ì œê±°
            if o1 in o2 and len(o1) < len(o2):
                to_drop.add(e1["idx"])
            elif o2 in o1 and len(o2) < len(o1):
                to_drop.add(e2["idx"])

    final_lines = [l for idx, l in enumerate(unique_lines) if idx not in to_drop]
    return "\n".join(final_lines)


def drop_lines_not_in_source(source_text: str, report: str) -> str:
    """
    report ì•ˆ '- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ':' íŒ¨í„´ì—ì„œ 'ì›ë¬¸'ì´
    1) ì‹¤ì œ source_textì— ì™„ì „ ë™ì¼í•˜ê²Œ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ìœ ì§€
    2) ë„ì–´ì“°ê¸° normalize í›„ì—ë„ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì œê±°
    """
    if not report:
        return ""

    cleaned: List[str] = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':", re.UNICODE)

    normalized_src = (
        (source_text or "")
        .replace(" ", "")
        .replace("\n", "")
        .replace("\u200b", "")
        .strip()
    )

    for line in report.splitlines():
        s = line.strip()
        if not s:
            continue

        m = pattern.match(s)
        if not m:
            cleaned.append(s)
            continue

        original = m.group(1)

        # ì™„ì „ ë™ì¼ ë§¤ì¹­ í—ˆìš©
        if original in (source_text or ""):
            cleaned.append(s)
            continue

        # ë„ì–´ì“°ê¸° ì œê±° í›„ ë¹„êµ
        if original.replace(" ", "") in normalized_src:
            cleaned.append(s)
            continue

        # ê·¸ ì™¸ëŠ” drop
        continue

    return "\n".join(cleaned)


def drop_escape_false(report: str) -> str:
    """
    JSON / Markdown escapeë¡œ ì¸í•œ ì˜¤íƒ ì œê±°
    """
    if not report:
        return ""

    false_patterns = [
        r'\\\"',   # \"
        r"\\\'",   # \'
        r'\"/\"',
        r'\"/',
        r'/\"',
        r'\\`',
    ]

    cleaned: List[str] = []
    for line in report.splitlines():
        s = line.strip()
        if not s:
            continue

        if any(re.search(p, s) for p in false_patterns):
            # escape ë¬¸ìì—´ë¡œ ì¸í•œ ì˜¤íŒ â†’ ì œê±°
            continue

        cleaned.append(s)

    return "\n".join(cleaned)


def ensure_final_punctuation_error(text: str, report: str) -> str:
    """
    ë¬¸ë‹¨ ë§ˆì§€ë§‰ ë¬¸ì¥ì˜ ëì— ì¢…ê²°ë¶€í˜¸(. ? !)ê°€ ì—†ìœ¼ë©´
    reportì— í•´ë‹¹ ì˜¤ë¥˜ë¥¼ ê°•ì œë¡œ í•œ ì¤„ ì¶”ê°€í•œë‹¤. (ì£¼ë¡œ í•œêµ­ì–´ìš©)
    """
    if not text or not text.strip():
        return report or ""

    s = text.rstrip()
    if not s:
        return report or ""

    last = s[-1]

    end_ok = False
    if last in ".?!":
        end_ok = True
    elif (
        len(s) >= 2
        and last in ['"', "'", "â€", "â€™", "ã€", "ã€", "ã€‹", "ã€‰", ")", "]"]
        and s[-2] in ".?!"
    ):
        end_ok = True

    if end_ok:
        return report or ""

    # ì´ë¯¸ ë¹„ìŠ·í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
    if report and ("ë§ˆì¹¨í‘œ" in report or "ë¬¸ì¥ë¶€í˜¸" in report):
        return report

    line = "- ë¬¸ë‹¨ ë§ˆì§€ë§‰ ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œ(ë˜ëŠ” ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ)ê°€ ë¹ ì ¸ ìˆìœ¼ë¯€ë¡œ ì ì ˆí•œ ë¬¸ì¥ë¶€í˜¸ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤."
    if report:
        return report.rstrip() + "\n" + line
    else:
        return line


def ensure_sentence_end_punctuation(text: str, report: str) -> str:
    """
    ë¬¸ë‹¨ ì•ˆì˜ ë¬¸ì¥ë“¤ ì¤‘ ì¢…ê²°ë¶€í˜¸ ì—†ëŠ” ë¬¸ì¥ì´ ìˆìœ¼ë©´ í•œ ì¤„ ìš”ì•½ ì˜¤ë¥˜ë¥¼ ì¶”ê°€.
    (í•œêµ­ì–´/ì˜ì–´ ê³µí†µ)
    """
    if not text or not text.strip():
        return report or ""

    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    missing = []

    for s in sentences:
        s = s.strip()
        if not s:
            continue

        ok = (
            s[-1] in ".?!"
            or (
                len(s) >= 2
                and s[-1] in ['"', "'", "â€", "â€™", "ã€", "ã€", "ã€‹", "ã€‰", ")", "]"]
                and s[-2] in ".?!"
            )
        )
        if not ok:
            missing.append(s)

    if not missing:
        return report or ""

    line = "- ë¬¸ì¥ ëì— ì¢…ê²°ë¶€í˜¸(., ?, !)ê°€ ëˆ„ë½ëœ ë¬¸ì¥ì´ ìˆìŠµë‹ˆë‹¤."
    if report:
        return report.rstrip() + "\n" + line
    else:
        return line


def clean_self_equal_corrections(report: str) -> str:
    """
    '- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ...' í˜•ì‹ì—ì„œ
    ì›ë¬¸ê³¼ ìˆ˜ì •ì•ˆì´ ì™„ì „íˆ ê°™ì€ ì¤„ì€ ì œê±°í•œë‹¤.
    (ì£¼ë¡œ ì˜ì–´ ìª½ content_typo_reportì— ì‚¬ìš©)
    """
    if not report:
        return ""

    cleaned_lines: List[str] = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':", re.UNICODE)

    for line in report.splitlines():
        line_stripped = line.strip()
        if not line_stripped:
            continue

        m = pattern.match(line_stripped)
        if not m:
            cleaned_lines.append(line_stripped)
            continue

        orig = m.group(1).strip()
        fixed = m.group(2).strip()

        if orig == fixed:
            continue

        cleaned_lines.append(line_stripped)

    return "\n".join(cleaned_lines)


def drop_false_period_errors(english_text: str, report: str) -> str:
    """
    ì˜ì–´ ì›ë¬¸ ëì— ì‹¤ì œë¡œ . ? ! ì´ ìˆìœ¼ë©´
    ë¦¬í¬íŠ¸ì—ì„œ 'ë§ˆì¹¨í‘œ ì—†ìŒ'ë¥˜ ë¬¸ì¥ì„ ì œê±°.
    (ê±°ì§“ ì–‘ì„± ì¤„ì´ê¸°ìš©) â€” ì£¼ë¡œ ë‹¨ì¼ ë¬¸ì¥ìš©
    """
    if not report:
        return ""

    stripped = (english_text or "").rstrip()
    last_char = stripped[-1] if stripped else ""

    if last_char in [".", "?", "!"]:
        bad_phrases = [
            "ë§ˆì¹¨í‘œê°€ ì—†ìŠµë‹ˆë‹¤",
            "ë§ˆì¹¨í‘œê°€ ë¹ ì ¸",
            "ë§ˆì¹¨í‘œê°€ í•„ìš”",
            "ë§ˆì¹¨í‘œë¥¼ ì°ì–´ì•¼",
            "Missing end-of-sentence punctuation",
        ]
        cleaned_lines = []
        for line in report.splitlines():
            if any(p in line for p in bad_phrases):
                continue
            cleaned_lines.append(line.strip())
        return "\n".join(cleaned_lines)

    return report


def split_report_by_source(report: str, plain_text: str, md_text: str) -> tuple[str, str]:
    """
    í•˜ë‚˜ì˜ ë¦¬í¬íŠ¸ë¥¼ 'ì›ë¬¸ì´ plainì—ì„œ ì˜¨ ê²ƒ' / 'ì›ë¬¸ì´ markdownì—ì„œ ì˜¨ ê²ƒ'ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.
    - "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ..." íŒ¨í„´ ê¸°ì¤€ìœ¼ë¡œ 'ì›ë¬¸'ì„ ë³´ê³  ì†Œì†ì„ ê²°ì •
    - ì›ë¬¸ì´ plainì—ë„ ìˆê³  mdì—ë„ ìˆìœ¼ë©´, ìš°ì„  plain ìª½ìœ¼ë¡œ ë³´ë‚¸ë‹¤.
    """
    if not report:
        return "", ""

    plain_lines: List[str] = []
    md_lines: List[str] = []

    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':.*", re.UNICODE)

    for line in report.splitlines():
        s = line.strip()
        if not s:
            continue

        m = pattern.match(s)
        if not m:
            # íŒ¨í„´ì´ ì•„ë‹ˆë©´ ì¼ë‹¨ plain ìª½ì— ë„£ì–´ë‘”ë‹¤
            plain_lines.append(s)
            continue

        original = m.group(1)

        in_plain = original in (plain_text or "")
        in_md = original in (md_text or "")

        if in_plain and not in_md:
            plain_lines.append(s)
        elif in_md and not in_plain:
            md_lines.append(s)
        else:
            # ë‘˜ ë‹¤ í¬í•¨ë˜ê±°ë‚˜ ë‘˜ ë‹¤ ì•ˆ í¬í•¨ë˜ë©´ ìš°ì„  plainìœ¼ë¡œ
            plain_lines.append(s)

    return "\n".join(plain_lines), "\n".join(md_lines)


# ---------------------------------------------------
# 4-1. ì¶”ê°€ í•„í„°: self equal ì œê±° (ê³µí†µ)
# ---------------------------------------------------

def remove_self_equal(report: str) -> str:
    """
    ëª¨ë“  ë¦¬í¬íŠ¸ì— ê³µí†µ ì ìš©í•˜ëŠ” self equal ì œê±°
    """
    if not report:
        return ""

    cleaned: List[str] = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':")

    for line in report.splitlines():
        m = pattern.match(line.strip())
        if m:
            orig = m.group(1).strip()
            fixed = m.group(2).strip()
            if orig == fixed:
                continue
        cleaned.append(line.strip())

    return "\n".join(cleaned)


def drop_language_switch(report: str) -> str:
    """
    í•œêµ­ì–´ ì›ë¬¸ â†’ ì˜ì–´ ìˆ˜ì •ì•ˆ / ì˜ì–´ ì›ë¬¸ â†’ í•œêµ­ì–´ ìˆ˜ì •ì•ˆ
    ë“± ì–¸ì–´ ìì²´ê°€ ë°”ë€Œë©´ ë¬´ì¡°ê±´ hallucinationìœ¼ë¡œ ì œê±°.
    """
    if not report:
        return ""

    cleaned: List[str] = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':")

    for line in report.splitlines():
        s = line.strip()
        m = pattern.match(s)

        if not m:
            cleaned.append(s)
            continue

        orig, fix = m.group(1), m.group(2)

        hangul_o = contains_hangul(orig)
        hangul_f = contains_hangul(fix)
        latin_o = contains_latin(orig)
        latin_f = contains_latin(fix)

        # í•œêµ­ì–´ â†’ ì˜ì–´ / ì˜ì–´ â†’ í•œêµ­ì–´ â†’ ë¬´ì¡°ê±´ ì‚­ì œ
        if hangul_o and latin_f:
            continue
        if latin_o and hangul_f:
            continue

        cleaned.append(s)

    return "\n".join(cleaned)


def drop_large_edits(report: str) -> str:
    """
    ìˆ˜ì •ì•ˆì´ ì›ë¬¸ë³´ë‹¤ 3ê¸€ì ì´ìƒ ë” ê¸¸ê±°ë‚˜ ì§§ìœ¼ë©´
    'ì˜ë¯¸ ë³€ê²½' ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ drop.
    (ê³µë°± ì œê±° í›„ ê¸¸ì´ ê¸°ì¤€)
    """
    if not report:
        return ""

    cleaned: List[str] = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':")

    for line in report.splitlines():
        s = line.strip()
        m = pattern.match(s)

        if not m:
            cleaned.append(s)
            continue

        orig = m.group(1).replace(" ", "")
        fix = m.group(2).replace(" ", "")

        if abs(len(fix) - len(orig)) >= 3:
            continue

        cleaned.append(s)

    return "\n".join(cleaned)


def drop_false_period_claims(text: str, report: str) -> str:
    """
    ì‹¤ì œë¡œ ì›ë¬¸ ì¡°ê° ëì— ì¢…ê²°ë¶€í˜¸ê°€ ìˆëŠ”ë°
    'ë§ˆì¹¨í‘œ ì—†ìŒ' / 'Missing end-of-sentence punctuation'ì´ë¼ê³  í•œ ì¤„ ì œê±°.
    (ì˜/í•œ ê³µí†µ)
    """
    if not report:
        return ""

    cleaned: List[str] = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':")

    false_keywords = [
        "Missing end-of-sentence punctuation",
        "sentence-ending punctuation",
        "ë§ˆì¹¨í‘œê°€ ì—†ìŠµë‹ˆë‹¤",
        "ë§ˆì¹¨í‘œê°€ í•„ìš”",
        "ë§ˆì¹¨í‘œê°€ ë¹ ì ¸",
        "ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œê°€ ì—†",
    ]

    for line in report.splitlines():
        s = line.strip()

        if not any(k in s for k in false_keywords):
            cleaned.append(s)
            continue

        m = pattern.match(s)
        if not m:
            cleaned.append(s)
            continue

        orig = m.group(1).rstrip()

        if not orig:
            cleaned.append(s)
            continue

        # ì›ë¬¸ ëì´ ì¢…ê²°ë¶€í˜¸ë©´ â†’ ì´ ì¤„ì€ ì˜¤íƒ
        if orig.endswith(('.', '?', '!')):
            continue
        if (
            len(orig) >= 2
            and orig[-1] in ['"', "'", "â€", "â€™", "ã€", "ã€", "]"]
            and orig[-2] in ".?!"
        ):
            continue

        cleaned.append(s)

    return "\n".join(cleaned)


def drop_punctuation_space_style(report: str) -> str:
    """
    'ë¬¸ì¥ë¶€í˜¸ ë’¤ ê³µë°±' ê°™ì€ ìŠ¤íƒ€ì¼ ì§€ì  ì „ë¶€ ì œê±°
    """
    if not report:
        return ""

    keywords = [
        "Missing space after",
        "space after punctuation",
        "space after the punctuation",
        "ê³µë°±ì´ í•„ìš”",
        "ê³µë°±ì„ ì¶”ê°€í•´ì•¼",
        "space after the sentence-ending punctuation mark",
    ]

    cleaned: List[str] = []
    for line in report.splitlines():
        if any(k in line for k in keywords):
            continue
        cleaned.append(line.strip())

    return "\n".join(cleaned)


def drop_false_whitespace_claims(text: str, report: str) -> str:
    """
    'ë¶ˆí•„ìš”í•œ ê³µë°±/ë„ì–´ì“°ê¸°' ì§€ì ì´ì§€ë§Œ ì›ë¬¸ ì¡°ê°ì— ì‹¤ì œ ê³µë°±/ì œë¡œí­ ê³µë°±ì´ ì—†ìœ¼ë©´ ì œê±°.
    """
    if not report:
        return ""

    cleaned: List[str] = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':.*(ë¶ˆí•„ìš”í•œ ê³µë°±|ë„ì–´ì“°ê¸°|ê³µë°±)", re.UNICODE)

    for line in report.splitlines():
        s = line.strip()
        if not s:
            continue

        m = pattern.match(s)
        if not m:
            cleaned.append(s)
            continue

        original = m.group(1)
        if not re.search(r"[ \t\u3000\u200b\u200c\u200d]", original):
            # ì‹¤ì œ ê³µë°±ì´ ì—†ìœ¼ë©´ ì˜¤íƒìœ¼ë¡œ ê°„ì£¼
            continue

        cleaned.append(s)

    return "\n".join(cleaned)


def sanitize_report(original_text: str, report: str) -> str:
    """
    ëª¨ë“  í›„ì²˜ë¦¬ í•„í„°ë¥¼ ìˆœì°¨ ì ìš©.
    (ì–¸ì–´ ìŠ¤ìœ„ì¹˜/ëŒ€ê·œëª¨ ìˆ˜ì •/escape/ë§ˆì¹¨í‘œ ì˜¤íƒ/ê³µë°± ìŠ¤íƒ€ì¼/í…ìŠ¤íŠ¸ ë¶ˆì¼ì¹˜ ë“±)
    """
    if not report:
        return ""

    r = report

    # 1) self equal ì œê±°
    r = remove_self_equal(r)

    # 2) escape ê¸°ë°˜ ì˜¤íƒ ì œê±°
    r = drop_escape_false(r)

    # 3) ì–¸ì–´ ìŠ¤ìœ„ì¹˜ ì œê±°
    r = drop_language_switch(r)

    # 4) í° í­ ìˆ˜ì • ì œê±°
    r = drop_large_edits(r)

    # 5) ë§ˆì¹¨í‘œ ì¡´ì¬í•˜ëŠ”ë° 'ì—†ë‹¤'ê³  ì£¼ì¥ â†’ ì œê±°
    r = drop_false_period_claims(original_text, r)

    # 6) ë§ˆì¹¨í‘œ/ì‰¼í‘œ ë’¤ ê³µë°± ìŠ¤íƒ€ì¼ ì§€ì  ì œê±°
    r = drop_punctuation_space_style(r)

    # 6-1) ì‹¤ì œ ê³µë°±ì´ ì—†ëŠ” ë° 'ë¶ˆí•„ìš”í•œ ê³µë°±/ë„ì–´ì“°ê¸°' ì§€ì  â†’ ì œê±°
    r = drop_false_whitespace_claims(original_text, r)

    # 7) ì›ë¬¸ì— ì—†ëŠ” ì¡°ê° ì œê±° (ë„ì–´ì“°ê¸° normalize ê¸°ë°˜)
    r = drop_lines_not_in_source(original_text, r)

    return r.strip()


# ---------------------------------------------------
# 5. í”„ë¡¬í”„íŠ¸ ì •ì˜ (ì˜ì–´ / í•œêµ­ì–´ ë¶„ë¦¬)
# ---------------------------------------------------

def create_english_review_prompt(text: str) -> str:
    """
    ì‹œíŠ¸ì˜ content(ì˜ì–´ ì›ë¬¸ + ë§ˆí¬ë‹¤ìš´)ì— ëŒ€í•´ ê²€ìˆ˜í•˜ëŠ” í”„ë¡¬í”„íŠ¸.
    - ìŠ¤í ë§ / split-word / AIâ†”Al / ëŒ€ë¬¸ì / ê¸°ë³¸ ë¬¸ì¥ ë¶€í˜¸
    - ê²°ê³¼ëŠ” content_typo_report(í•œêµ­ì–´ ì„¤ëª…)ì—ë§Œ ìŒ“ì´ê²Œ ìœ ë„
    """
    return f"""
You are a machine-like **English text proofreader**.
Your ONLY job is to detect **objective, verifiable errors** in the following English text.
You MUST NOT suggest stylistic changes, paraphrasing, natural-sounding alternatives,
tone changes, or meaning changes.

ğŸš« DO NOT change meaning/order/length
- Do NOT rewrite, summarize, or rephrase any sentence.
- Do NOT add/remove/replace tokens unless it's the minimal fix for a spelling/spacing/punctuation error.
- Keep numbers, symbols, and structure exactly as in the original.

Your response MUST be a single valid JSON object with keys:
- "suspicion_score": integer 1~5
- "content_typo_report": string (Korean ì„¤ëª…)
- "translated_typo_report": ""   â† í•­ìƒ ë¹ˆ ë¬¸ìì—´
- "markdown_report": ""          â† í•­ìƒ ë¹ˆ ë¬¸ìì—´

All explanations in content_typo_report MUST be written in **Korean**.

If there are no errors:
- suspicion_score = 1
- all reports = ""

------------------------------------------------------------
# IMPORTANT ANTI-HALLUCINATION RULE
------------------------------------------------------------
- In the pattern "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ...", the 'ì›ë¬¸' part MUST be a substring
  that actually appears in the input text `plain_english`.
- "ì›ë¬¸" MUST always be copied from `plain_english` exactly as it appears.
- You MUST NOT invent new tokens or reuse example tokens that do not literally
  appear in the input text.

------------------------------------------------------------
# 1. RULES FOR ENGLISH OBJECTIVE ERRORS
------------------------------------------------------------

## (A) Split-Word Errors (í•­ìƒ ì˜¤íƒ€ë¡œ ì·¨ê¸‰ â€” ë§¤ìš° ì¤‘ìš”)
If an English word appears with an incorrect internal space,
AND removing the space yields a valid English word,
you MUST treat it as a spelling error.

## (B) Normal English spelling mistakes (MUST detect)
Any token similar to a valid English word (1â€“2 letters swapped/missing) MUST be flagged.

## (C) AI ë¬¸ë§¥ì—ì„œ "Al" â†’ "AI" (í•­ìƒ ì¡ê¸°)
If the surrounding sentence mentions:
model / system / tool / chatbot / LLM / agent / dataset / training / inference
then â€œAlâ€ (A+ì†Œë¬¸ì l) MUST be interpreted as a typo for â€œAIâ€.

## (D) Capitalization Errors
- Sentence starting with lowercase
- Pronoun â€œIâ€ written as â€œiâ€
- Proper nouns not capitalized (london â†’ London)

## (E) Duplicate / spacing errors
- "the the"
- "re turn" â†’ "return"
- "mod el" â†’ "model"

## (F) STRICT punctuation rule â€” avoid false positives
You MUST NOT report a punctuation error if the text already ends with ANY of:
- ".", "?", "!"
- '."' / '!"' / '?"'
- ".â€™" / "!â€™" / "?â€™"

ONLY report a punctuation error if:
- the sentence has NO ending punctuation at all, OR
- a closing quotation mark is missing, OR
- punctuation is clearly malformed (e.g. ",.", ".,", "..", "!!", "??" in a wrong place)

STRICT length rule:
- If a suggested fix changes the length of the original token by 3 or more characters (after removing spaces), DO NOT report it.

------------------------------------------------------------
# 2. OUTPUT FORMAT
------------------------------------------------------------
You MUST output EXACTLY ONE JSON object (no extra text, no markdown).

Each error line example (in Korean):

"- 'understaning' â†’ 'understanding': 'understaning'ì€ ì² ì ì˜¤íƒ€ì´ë©° 'understanding'ìœ¼ë¡œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤."

If there is NO objective error at all:
- "suspicion_score": 1
- "content_typo_report": ""
- "translated_typo_report": ""
- "markdown_report": ""

------------------------------------------------------------
# 3. TEXT TO REVIEW
plain_english: \"\"\"{text}\"\"\"
"""


def create_korean_review_prompt(text: str) -> str:
    """
    ì‹œíŠ¸ì˜ content_translated(í•œêµ­ì–´ ë²ˆì—­ + ë§ˆí¬ë‹¤ìš´)ì— ëŒ€í•´ ê²€ìˆ˜í•˜ëŠ” í”„ë¡¬í”„íŠ¸.
    - ì˜¤íƒˆì / ì¡°ì‚¬Â·ì–´ë¯¸ / ë„ì–´ì“°ê¸° / í˜•íƒœì†Œ ë¶„ë¦¬ / ë°˜ë³µ / ë¬¸ì¥ë¶€í˜¸
    - ê²°ê³¼ëŠ” translated_typo_reportì—ë§Œ ìŒ“ì´ê²Œ ìœ ë„
    """
    return f"""
ë‹¹ì‹ ì€ ê¸°ê³„ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” **Korean text proofreader**ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ìœ ì¼í•œ ì„ë¬´ëŠ” ì•„ë˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ **ê°ê´€ì ì´ê³  í™•ì¸ ê°€ëŠ¥í•œ ì˜¤ë¥˜ë§Œ** ì°¾ì•„ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤.
ìŠ¤íƒ€ì¼, ì–´íˆ¬, ìì—°ìŠ¤ëŸ¬ì›€, í‘œí˜„ ê°œì„ , ì˜ë„ ì¶”ë¡ ê³¼ ê°™ì€ ì£¼ê´€ì  íŒë‹¨ì€ ì ˆëŒ€ í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.

ì¶œë ¥ì€ ë°˜ë“œì‹œ ì•„ë˜ 4ê°œì˜ keyë§Œ í¬í•¨í•˜ëŠ” **ë‹¨ì¼ JSON ê°ì²´**ì—¬ì•¼ í•©ë‹ˆë‹¤.
- "suspicion_score": 1~5 ì •ìˆ˜
- "content_typo_report": ""       â† í•­ìƒ ë¹ˆ ë¬¸ìì—´ (ì˜ì–´ìš© í•„ë“œ)
- "translated_typo_report": í•œêµ­ì–´ ì˜¤ë¥˜ ì„¤ëª… (ì—†ìœ¼ë©´ "")
- "markdown_report": ""           â† í•­ìƒ ë¹ˆ ë¬¸ìì—´

ëª¨ë“  ì„¤ëª…ì€ ë°˜ë“œì‹œ **í•œêµ­ì–´ë¡œ** ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
ì˜¤ë¥˜ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ëª¨ë“  report í•„ë“œëŠ” "" ì—¬ì•¼ í•©ë‹ˆë‹¤.

============================================================
ğŸš¨ ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™ (ì›ë¬¸ ë³´ì¡´ â€” ì ˆëŒ€ ìœ„ë°˜ ê¸ˆì§€)
============================================================
ì•„ë˜ ì§ˆë¬¸ ì¤‘ í•˜ë‚˜ë¼ë„ â€œì˜ˆâ€ë¼ë©´, ê·¸ ìˆ˜ì •ì€ **ë³´ê³ í•˜ì§€ ë§ê³  ì™„ì „íˆ ë¬´ì‹œ**í•´ì•¼ í•©ë‹ˆë‹¤.

1) ìˆ˜ì •í•˜ë ¤ëŠ” ë¶€ë¶„ì´ plain_koreanì— **ê·¸ëŒ€ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ê°€?**
2) ìˆ˜ì •í•˜ë ¤ë©´ **5ê¸€ì ì´ìƒ** ë°”ê¿”ì•¼ í•˜ëŠ”ê°€?
3) ë‹¨ì–´ **ìˆœì„œë¥¼ ë³€ê²½**í•´ì•¼ í•˜ëŠ”ê°€?
4) ì˜ë¯¸ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆëŠ” ìˆ˜ì •ì¸ê°€?
5) ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ **ì¶”ê°€í•´ì•¼ë§Œ** ìˆ˜ì •ì´ ê°€ëŠ¥í•œê°€?
6) ìì—°ìŠ¤ëŸ½ê²Œ ë“¤ë¦¬ë„ë¡ **ë‹¤ë“¬ëŠ” ê²ƒ**ì²˜ëŸ¼ ë³´ì´ëŠ”ê°€?
7) ë¬¸ì¥ì„ ì‚¬ì‹¤ìƒ **ë‹¤ì‹œ ì“°ëŠ” ê²ƒì²˜ëŸ¼** ë³´ì´ëŠ”ê°€?

â†’ í•˜ë‚˜ë¼ë„ â€œì˜ˆâ€ë¼ë©´, í•´ë‹¹ ì˜¤ë¥˜ëŠ” **ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.**

============================================================
ğŸš« Hallucination ë°©ì§€ ê·œì¹™
============================================================
âŒ ì…ë ¥ í…ìŠ¤íŠ¸ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´Â·êµ¬ì ˆ ìƒì„± ê¸ˆì§€  
âŒ í”„ë¡¬í”„íŠ¸ ì„¤ëª…ë¶€ì— ìˆëŠ” ë‹¨ì–´ë¥¼ â€˜ì›ë¬¸â€™ìœ¼ë¡œ ì¬ì‚¬ìš© ê¸ˆì§€  
âŒ ì›ë¬¸ì˜ ë¬¸ì¥ êµ¬ì¡°Â·ì˜ë„Â·í†¤Â·ì–´ìˆœ ë³€ê²½ ê¸ˆì§€  

'- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ...' í˜•ì‹ì˜ 'ì›ë¬¸'ì€  
ë°˜ë“œì‹œ plain_korean ì•ˆì— **ë¬¸ì ë‹¨ìœ„ë¡œ ë™ì¼í•˜ê²Œ ì¡´ì¬**í•´ì•¼ í•©ë‹ˆë‹¤.

============================================================
# 1. í•œêµ­ì–´ì—ì„œ ë°˜ë“œì‹œ ì¡ì•„ì•¼ í•˜ëŠ” ê°ê´€ì  ì˜¤ë¥˜
============================================================
(A) ì˜¤íƒˆì / ì² ì ì˜¤ë¥˜  
(B) ì¡°ì‚¬Â·ì–´ë¯¸ ì˜¤ë¥˜  
(C) ë‹¨ì–´ ë‚´ë¶€ ë¶ˆí•„ìš”í•œ ê³µë°±  
(D) ë°˜ë³µ ì˜¤íƒ€  
(E) ëª…ë°±í•œ ë„ì–´ì“°ê¸° ì˜¤ë¥˜  
(F) ë¬¸ì¥ë¶€í˜¸ ì˜¤ë¥˜  
   - ë¬¸ì¥ ëì— ì¢…ê²°ë¶€í˜¸ ì—†ìŒ  
   - ë”°ì˜´í‘œ ì§ ë¶ˆì¼ì¹˜  
   - ëª…ë°±íˆ ì˜ëª»ëœ ì‰¼í‘œ  
   - ë¬¸ì¥ ì¤‘ê°„ì˜ ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ/ì‰¼í‘œ  

[G] ë¬¸ì¥ë¶€í˜¸ ë’¤ ê³µë°± ê·œì¹™ (ì¤‘ìš”)
- ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œê°€ ìˆê³ , ê·¸ ë’¤ì—ì„œ ìƒˆë¡œìš´ ë¬¸ì¥ì´ ì‹œì‘ë  ê²½ìš°,
  ë¬¸ì¥ë¶€í˜¸ ë’¤ì˜ ê³µë°±ì€ **ì •ìƒì´ë©° ì˜¤íƒ€ê°€ ì•„ë‹ˆë‹¤.**
- ë‹¨ì–´ ë‚´ë¶€ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±(ì˜ˆ: 'í˜ ë¦°ë‹¤', 'ëœ ë‹¤')ë§Œ ì˜¤ë¥˜ë¡œ ì¸ì •í•œë‹¤.

============================================================
# 2. OUTPUT FORMAT (JSON Only)
============================================================
ì˜¤ë¥˜ê°€ ìˆì„ ê²½ìš° í•œ ì¤„ì”© bullet:

"- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì˜¤ë¥˜ ì„¤ëª…"

ì˜¤ë¥˜ê°€ ì—†ë‹¤ë©´:
- suspicion_score = 1
- content_typo_report = ""
- translated_typo_report = ""
- markdown_report = ""

============================================================
# 3. TEXT TO REVIEW
============================================================
plain_korean: \"\"\"{text}\"\"\"
"""


# ---------------------------------------------------
# 6. Gemini í˜¸ì¶œ / ê¸°ë³¸ ì •ì œ
# ---------------------------------------------------

def analyze_text_with_gemini(prompt: str, max_retries: int = 5) -> dict:
    """Geminië¥¼ JSON ëª¨ë“œë¡œ í˜¸ì¶œ + ì¬ì‹œë„ ë¡œì§"""
    last_error: Exception | None = None

    for attempt in range(max_retries):
        try:
            generation_config = {
                "response_mime_type": "application/json",
                "temperature": 0.0,
            }
            response = model.generate_content(
                prompt,
                generation_config=generation_config,
            )
            return json.loads(response.text)

        except Exception as e:
            last_error = e
            wait_time = 5 * (attempt + 1)
            print(f"Gemini í˜¸ì¶œ ì˜¤ë¥˜ (ì‹œë„ {attempt+1}/{max_retries}): {e}")

            if attempt < max_retries - 1:
                print(f"â†’ {wait_time}ì´ˆ í›„ ì¬ì‹œë„")
                time.sleep(wait_time)

    print("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼.")
    return {
        "suspicion_score": 5,
        "content_typo_report": f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(last_error)}",
        "translated_typo_report": "",
        "markdown_report": "",
    }


def validate_and_clean_analysis(result: dict) -> dict:
    """
    ëª¨ë¸ ì‘ë‹µì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ë³´ì • + ìŠ¤íƒ€ì¼/ë¬¸ì²´ì„± ë©˜íŠ¸ í•„í„°ë§.
    """
    # 0) Geminiê°€ ì—ëŸ¬ JSONì„ ëŒë ¤ì¤€ ê²½ìš° ì²˜ë¦¬
    if isinstance(result, dict) and "ERROR" in result:
        err_obj = result.get("ERROR") or {}
        if isinstance(err_obj, dict):
            msg = err_obj.get("message") or str(err_obj)
        else:
            msg = str(err_obj)

        return {
            "suspicion_score": 5,
            "content_typo_report": f"Gemini API ë‚´ë¶€ ì˜¤ë¥˜: {msg}",
            "translated_typo_report": "",
            "markdown_report": "",
        }

    # 1) ì•„ì˜ˆ dictê°€ ì•„ë‹ ë•Œ
    if not isinstance(result, dict):
        return {
            "suspicion_score": 5,
            "content_typo_report": "AI ì‘ë‹µì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹˜",
            "translated_typo_report": "",
            "markdown_report": "",
        }

    score = result.get("suspicion_score")
    reports = {
        "content_typo_report": result.get("content_typo_report", "") or "",
        "translated_typo_report": result.get("translated_typo_report", "") or "",
        "markdown_report": result.get("markdown_report", "") or "",
    }

    # ìŠ¤íƒ€ì¼/ë¬¸ì²´ ì œì•ˆ ê¸ˆì§€ í‚¤ì›Œë“œ í•„í„°
    forbidden_keywords = [
        "ë¬¸ë§¥ìƒ",
        "ë¶€ì ì ˆ",
        "ì–´ìƒ‰",
        "ë” ìì—°ìŠ¤ëŸ½",
        "ë” ì ì ˆ",
        "ìˆ˜ì •í•˜ëŠ” ê²ƒì´ ì¢‹",
        "ì œì•ˆ",
        "ë°”ê¾¸ëŠ” ê²ƒ",
        "ì˜ë¯¸ë¥¼ ëª…í™•íˆ",
    ]
    for key, text in reports.items():
        if any(kw in text for kw in forbidden_keywords):
            reports[key] = ""

    # "ì˜¤ë¥˜ ì—†ìŒ"ë¥˜ ë©˜íŠ¸ ì œê±°
    forbidden_phrases = ["ì˜¤ë¥˜ ì—†ìŒ", "ì •ìƒ", "ë¬¸ì œ ì—†ìŒ", "ìˆ˜ì •í•  í•„ìš” ì—†ìŒ"]
    for key, text in reports.items():
        if any(ph in text for ph in forbidden_phrases):
            reports[key] = ""

    # ì˜ì–´ ë¦¬í¬íŠ¸ì— ëŒ€í•´ì„œ self equal ì •ë¦¬
    reports["content_typo_report"] = clean_self_equal_corrections(
        reports["content_typo_report"]
    )

    # score ê¸°ë³¸ê°’ ë³´ì •
    try:
        score = int(score)
    except Exception:
        score = 1

    if score < 1:
        score = 1
    if score > 5:
        score = 5

    if (
        not reports["content_typo_report"]
        and not reports["translated_typo_report"]
        and not reports["markdown_report"]
    ):
        score = 1
    elif (
        (reports["content_typo_report"] or reports["translated_typo_report"] or reports["markdown_report"])
        and score == 1
    ):
        score = 3

    return {
        "suspicion_score": score,
        "content_typo_report": reports["content_typo_report"],
        "translated_typo_report": reports["translated_typo_report"],
        "markdown_report": reports["markdown_report"],
    }


# ---------------------------------------------------
# 7. í•œ í–‰(ì˜ì–´+í•œêµ­ì–´)ì„ í†µí•© ê²€ìˆ˜í•˜ëŠ” í—¬í¼
# ---------------------------------------------------

def analyze_row_with_both_langs(row: Dict[str, Any]):
    """
    í•œ í–‰(row)ì— ëŒ€í•´:
      - content / content_markdown (ì˜ì–´)
      - content_translated / content_markdown_translated (í•œêµ­ì–´)
    ë¥¼ ëª¨ë‘ í•©ì³ì„œ í•œ ë²ˆì— ê²€ìˆ˜í•œë‹¤.
    """

    # 1) ì›ë³¸ í…ìŠ¤íŠ¸ë“¤ ê°€ì ¸ì˜¤ê¸°
    en_plain = (row.get(ORIGINAL_TEXT_COL) or "").strip()
    en_md = (row.get(ORIGINAL_MD_COL) or "").strip()
    ko_plain = (row.get(TRANSLATION_TEXT_COL) or "").strip()
    ko_md = (row.get(TRANSLATION_MD_COL) or "").strip()

    # 2) ì‹¤ì œë¡œ ëª¨ë¸ì— ë³´ë‚¼ í†µí•© í…ìŠ¤íŠ¸ (ë¹ˆ ê±´ ì œì™¸í•˜ê³  ì¤„ë°”ê¿ˆìœ¼ë¡œ ì´ì–´ ë¶™ì´ê¸°)
    en_text = "\n".join(t for t in [en_plain, en_md] if t)
    ko_text = "\n".join(t for t in [ko_plain, ko_md] if t)

    raw_en = final_en = None
    raw_ko = final_ko = None

    # --- ì˜ì–´ ìª½ ---
    if en_text:
        prompt_en = create_english_review_prompt(en_text)
        raw_en = analyze_text_with_gemini(prompt_en)
        final_en = validate_and_clean_analysis(raw_en)

        filtered_en = sanitize_report(
            en_text,
            final_en.get("content_typo_report", "") or "",
        )
        # ì˜ì–´ ìª½ë„ ë¬¸ì¥ ë ì¢…ê²°ë¶€í˜¸ ëˆ„ë½ì„ í•œ ì¤„ ìš”ì•½ìœ¼ë¡œ catch
        filtered_en = ensure_sentence_end_punctuation(en_text, filtered_en)
        final_en["content_typo_report"] = filtered_en
    else:
        final_en = {
            "suspicion_score": 1,
            "content_typo_report": "",
            "translated_typo_report": "",
            "markdown_report": "",
        }

    # --- í•œêµ­ì–´ ìª½ ---
    if ko_text:
        prompt_ko = create_korean_review_prompt(ko_text)
        raw_ko = analyze_text_with_gemini(prompt_ko)
        final_ko = validate_and_clean_analysis(raw_ko)

        filtered_ko = sanitize_report(
            ko_text,
            final_ko.get("translated_typo_report", "") or "",
        )
        filtered_ko = ensure_final_punctuation_error(ko_text, filtered_ko)
        filtered_ko = ensure_sentence_end_punctuation(ko_text, filtered_ko)
        filtered_ko = dedup_korean_bullet_lines(filtered_ko)
        final_ko["translated_typo_report"] = filtered_ko
    else:
        final_ko = {
            "suspicion_score": 1,
            "content_typo_report": "",
            "translated_typo_report": "",
            "markdown_report": "",
        }

    # --- plain / markdown ê¸°ì¤€ìœ¼ë¡œ ë¦¬í¬íŠ¸ ë¶„ë¦¬ ---
    raw_en_report = ""
    raw_ko_report = ""
    if isinstance(raw_en, dict):
        raw_en_report = raw_en.get("content_typo_report", "") or ""
    if isinstance(raw_ko, dict):
        raw_ko_report = raw_ko.get("translated_typo_report", "") or ""

    raw_en_plain_report, raw_en_md_report = split_report_by_source(
        raw_en_report,
        en_plain,
        en_md,
    )
    raw_ko_plain_report, raw_ko_md_report = split_report_by_source(
        raw_ko_report,
        ko_plain,
        ko_md,
    )

    en_plain_report, en_md_report = split_report_by_source(
        final_en.get("content_typo_report", "") or "",
        en_plain,
        en_md,
    )
    ko_plain_report, ko_md_report = split_report_by_source(
        final_ko.get("translated_typo_report", "") or "",
        ko_plain,
        ko_md,
    )

    # plain ìª½ì€ ê¸°ì¡´ ì»¬ëŸ¼ì— ë‚¨ê¸°ê³ 
    final_en["content_typo_report"] = en_plain_report
    final_ko["translated_typo_report"] = ko_plain_report

    # markdownì—ì„œ ë‚˜ì˜¨ ì˜¤ë¥˜ëŠ” MARKDOWN_REPORTë¡œ ëª¨ìœ¼ê¸°
    markdown_report_parts: List[str] = []
    if en_md_report:
        markdown_report_parts.append(en_md_report)
    if ko_md_report:
        markdown_report_parts.append(ko_md_report)
    markdown_report = "\n".join(markdown_report_parts)

    # --- í†µí•© ìŠ¤ì½”ì–´ ---
    combined_final = {
        "suspicion_score": max(
            final_en.get("suspicion_score", 1),
            final_ko.get("suspicion_score", 1),
        ),
        "content_typo_report": final_en.get("content_typo_report", ""),
        "translated_typo_report": final_ko.get("translated_typo_report", ""),
        "markdown_report": markdown_report,
    }

    debug_bundle = {
        "english": {
            "text_plain": en_plain,
            "text_markdown": en_md,
            "text": en_text,  # ì‹¤ì œë¡œ ê²€ìˆ˜í•œ í†µí•© í…ìŠ¤íŠ¸
            "raw_report_plain": raw_en_plain_report,
            "raw_report_markdown": raw_en_md_report,
            "report_plain": en_plain_report,
            "report_markdown": en_md_report,
            "raw": raw_en,
            "final": final_en,
        },
        "korean": {
            "text_plain": ko_plain,
            "text_markdown": ko_md,
            "text": ko_text,
            "raw_report_plain": raw_ko_plain_report,
            "raw_report_markdown": raw_ko_md_report,
            "report_plain": ko_plain_report,
            "report_markdown": ko_md_report,
            "raw": raw_ko,
            "final": final_ko,
        },
    }

    return combined_final, debug_bundle


# ---------------------------------------------------
# 8. ê³µê°œ í•¨ìˆ˜: ì‹œíŠ¸ ì „ì²´ë¥¼ ëŒë¦¬ê³  ìš”ì•½ ë¦¬í„´
# ---------------------------------------------------

def run_sheet_review(
    spreadsheet_name: str,
    worksheet_name: str,
    collect_raw: bool = False,
    progress_callback=None,
) -> dict:
    """
    - ì£¼ì–´ì§„ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ / ì›Œí¬ì‹œíŠ¸ì—ì„œ
    - STATUS == '1. AIê²€ìˆ˜ìš”ì²­' ì¸ í–‰ë§Œ ê³¨ë¼ì„œ
    - SCORE / *_REPORT / STATUSë¥¼ ì±„ì›Œë„£ëŠ”ë‹¤.

    ë°˜í™˜ê°’: {
      "total_rows": ...,
      "target_rows": ...,
      "processed_rows": ...,
      "raw_results": [  # collect_raw=Trueì¼ ë•Œë§Œ
          {
            "sheet_row_index": int,
            "english": {"text", "raw", "final"},
            "korean": {"text", "raw", "final"},
            "combined_final": {...},
          },
          ...
      ]
    }
    """
    try:
        spreadsheet = gs_client.open(spreadsheet_name)
    except gspread.exceptions.SpreadsheetNotFound:
        raise ValueError(f"ìŠ¤í”„ë ˆë“œì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {spreadsheet_name}")

    try:
        worksheet = spreadsheet.worksheet(worksheet_name)
    except gspread.exceptions.WorksheetNotFound:
        raise ValueError(f"ì›Œí¬ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {worksheet_name}")

    all_data = worksheet.get_all_records()
    df = pd.DataFrame(all_data)
    df["sheet_row_index"] = df.index + 2  # 1í–‰ì€ í—¤ë”ë¼ì„œ +2

    targets = df[df[STATUS_COL] == "1. AIê²€ìˆ˜ìš”ì²­"].copy()
    if targets.empty:
        return {
            "total_rows": len(df),
            "target_rows": 0,
            "processed_rows": 0,
            "raw_results": [],
        }

    results: List[Dict[str, Any]] = []
    raw_results: List[Dict[str, Any]] = []

    total_targets = len(targets)

    for i, (_, row) in enumerate(targets.iterrows(), start=1):
        row_dict = row.to_dict()
        row_idx = row["sheet_row_index"]
        print(f"í–‰ {row_idx} ê²€ìˆ˜ ì¤‘... ({i}/{total_targets})")

        if progress_callback is not None:
            progress_callback(i, total_targets)

        # ğŸ”¹ ì˜ì–´ + í•œêµ­ì–´ í†µí•© ê²€ìˆ˜
        combined_final, debug_bundle = analyze_row_with_both_langs(row_dict)

        results.append(
            {
                "sheet_row_index": row_idx,
                SUSPICION_SCORE_COL: combined_final.get("suspicion_score"),
                CONTENT_TYPO_REPORT_COL: combined_final.get("content_typo_report"),
                TRANSLATED_COL: combined_final.get("translated_typo_report"),
                MARKDOWN_REPORT_COL: combined_final.get("markdown_report"),
                STATUS_COL: "2. AIê²€ìˆ˜ì™„ë£Œ",
            }
        )

        if collect_raw:
            raw_results.append(
                {
                    "sheet_row_index": row_idx,
                    **debug_bundle,
                    "combined_final": combined_final,
                }
            )

        time.sleep(0.5)  # API ê³¼ë‹¤ í˜¸ì¶œ ë°©ì§€ìš© (í•„ìš”ì‹œ ì¡°ì •)

    # === ì‹œíŠ¸ì— ê²°ê³¼ ë°˜ì˜ ===
    headers = worksheet.row_values(1)
    score_col_idx = headers.index(SUSPICION_SCORE_COL) + 1
    content_col_idx = headers.index(CONTENT_TYPO_REPORT_COL) + 1
    translated_col_idx = headers.index(TRANSLATED_COL) + 1
    markdown_col_idx = headers.index(MARKDOWN_REPORT_COL) + 1
    status_col_idx = headers.index(STATUS_COL) + 1

    def sanitize_cell(v):
        return "" if v is None else str(v)

    update_cells: List[gspread.Cell] = []
    for r in results:
        ridx = r["sheet_row_index"]
        update_cells.append(
            gspread.Cell(ridx, score_col_idx, sanitize_cell(r[SUSPICION_SCORE_COL]))
        )
        update_cells.append(
            gspread.Cell(ridx, content_col_idx, sanitize_cell(r[CONTENT_TYPO_REPORT_COL]))
        )
        update_cells.append(
            gspread.Cell(ridx, translated_col_idx, sanitize_cell(r[TRANSLATED_COL]))
        )
        update_cells.append(
            gspread.Cell(ridx, markdown_col_idx, sanitize_cell(r[MARKDOWN_REPORT_COL]))
        )
        update_cells.append(
            gspread.Cell(ridx, status_col_idx, sanitize_cell(r[STATUS_COL]))
        )

    if update_cells:
        worksheet.update_cells(update_cells)

    return {
        "total_rows": len(df),
        "target_rows": len(targets),
        "processed_rows": len(results),
        "raw_results": raw_results,
    }

# sheet_review.py
# -*- coding: utf-8 -*-
import json
import time
import re
from typing import Dict, Any, List

import streamlit as st
import pandas as pd
import gspread
import google.generativeai as genai
from google.oauth2.service_account import Credentials

# ---------------------------------------------------
# 1. Gemini / Google Sheets í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
# ---------------------------------------------------

# Gemini í‚¤ (secretsì—ì„œ ì½ê¸°)
API_KEY = st.secrets.get("GEMINI_API_KEY")
if not API_KEY:
    st.error("GEMINI_API_KEYê°€ secretsì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

genai.configure(api_key=API_KEY)
MODEL_ID = "gemini-2.0-flash-001"
model = genai.GenerativeModel(MODEL_ID)

# ì„œë¹„ìŠ¤ ê³„ì • ì •ë³´ (JSON ì „ì²´ë¥¼ secretsì— ë„£ì–´ë‘ )
raw = st.secrets["GCP_SERVICE_ACCOUNT_JSON"]

if isinstance(raw, dict):
    service_info = dict(raw)
elif isinstance(raw, str):
    service_info = json.loads(raw)
else:
    st.error("GCP_SERVICE_ACCOUNT_JSON í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()

SCOPES = [
    "https://www.googleapis.com/auth/spreadsheets",
    "https://www.googleapis.com/auth/drive",
]

creds = Credentials.from_service_account_info(service_info, scopes=SCOPES)
gs_client = gspread.authorize(creds)


# ---------------------------------------------------
# 2. ì‹œíŠ¸ ì»¬ëŸ¼ ì´ë¦„ (ê¸°ì¡´ ìŠ¤í‚¤ë§ˆ)
# ---------------------------------------------------

STATUS_COL = "STATUS"
ORIGINAL_TEXT_COL = "content"                   # ì˜ì–´ ì›ë¬¸
ORIGINAL_MD_COL = "content_markdown"
TRANSLATION_TEXT_COL = "content_translated"    # í•œêµ­ì–´ ë²ˆì—­
TRANSLATION_MD_COL = "content_markdown_translated"

SUSPICION_SCORE_COL = "SCORE"
CONTENT_TYPO_REPORT_COL = "CONTENT_TYPO_REPORT"      # ì˜ì–´ ê²€ìˆ˜ ê²°ê³¼
TRANSLATED_COL = "TRANSLATED_TYPO_REPORT"            # í•œêµ­ì–´ ê²€ìˆ˜ ê²°ê³¼
MARKDOWN_REPORT_COL = "MARKDOWN_REPORT"              # ë§ˆí¬ë‹¤ìš´ ê´€ë ¨ ì˜¤ë¥˜


# ---------------------------------------------------
# 3. ê³µí†µ ìœ í‹¸: ë¦¬í¬íŠ¸ í›„ì²˜ë¦¬ / ë¬¸ì¥ë¶€í˜¸ ê°•ì œ / hallucination í•„í„°
# ---------------------------------------------------

def dedup_korean_bullet_lines(report: str) -> str:
    """
    í•œêµ­ì–´ bullet ë¦¬í¬íŠ¸ì—ì„œ ì˜ë¯¸ê°€ ê²¹ì¹˜ëŠ” ì¤„ì„ ì •ë¦¬í•œë‹¤.
    - ì™„ì „íˆ ë™ì¼í•œ ì¤„ì€ í•˜ë‚˜ë§Œ ë‚¨ê¹€
    - 'ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ'ë¥˜ì—ì„œ ì›ë¬¸ì´ ë¶€ë¶„ ë¬¸ìì—´ ê´€ê³„ì´ë©´ ë” ê¸´ ìª½ë§Œ ìœ ì§€
      (ì˜ˆ: 'í–ˆê³ .' vs 'ì—­í• ì„ í–ˆê³ .' -> í›„ìë§Œ ë‚¨ê¹€)
    """
    if not report:
        return ""

    lines = [l.strip() for l in report.splitlines() if l.strip()]
    if not lines:
        return ""

    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':\s*(.+)$", re.UNICODE)

    # 1ì°¨: ì™„ì „ ì¤‘ë³µ ì œê±°
    unique_lines = []
    seen = set()
    for l in lines:
        if l not in seen:
            unique_lines.append(l)
            seen.add(l)

    # 2ì°¨: ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ ê´€ë ¨ ì¤‘ë³µ ì œê±°
    entries = []
    for idx, l in enumerate(unique_lines):
        m = pattern.match(l)
        if not m:
            entries.append({"idx": idx, "raw": l, "orig": None, "fixed": None, "msg": ""})
            continue
        orig, fixed, msg = m.group(1), m.group(2), m.group(3)
        entries.append({"idx": idx, "raw": l, "orig": orig, "fixed": fixed, "msg": msg})

    to_drop = set()
    for i, e1 in enumerate(entries):
        if not e1["orig"] or "ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ" not in e1["msg"]:
            continue
        for j, e2 in enumerate(entries):
            if i == j or not e2["orig"] or "ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ" not in e2["msg"]:
                continue

            o1, o2 = e1["orig"], e2["orig"]
            # ë” ì§§ì€ ê²ƒì´ ë” ê¸´ ê²ƒì˜ ë¶€ë¶„ ë¬¸ìì—´ì´ë©´ ì§§ì€ ê²ƒ ì œê±°
            if o1 in o2 and len(o1) < len(o2):
                to_drop.add(e1["idx"])
            elif o2 in o1 and len(o2) < len(o1):
                to_drop.add(e2["idx"])

    final_lines = [
        l for idx, l in enumerate(unique_lines) if idx not in to_drop
    ]

    return "\n".join(final_lines)


# ì¢…ê²°ë¶€í˜¸ ë’¤ ê³µë°±ì€ ì •ìƒ â†’ ë³´ê³ ì„œì—ì„œ ì œê±°
def drop_false_punctuation_space_errors(text: str, report: str) -> str:
    """
    '- ...: Spacing error' ë¥˜ ì¤‘ì—ì„œ
    'ë¬¸ì¥ë¶€í˜¸(.?! ë“±) + ê³µë°± + ìƒˆ ë¬¸ì¥ ì‹œì‘' í˜•íƒœëŠ” ì •ìƒìœ¼ë¡œ ë³´ê³  ì œê±°.
    """
    if not report:
        return report

    fixed = []
    for line in report.splitlines():
        if "Spacing error" in line or "ê³µë°± ì˜¤ë¥˜" in line:
            # ì›ë¬¸ ì¶”ì¶œ
            m = re.search(r"'(.+?)' â†’", line)
            if m:
                original = m.group(1)
                # ì¢…ê²°ë¶€í˜¸ ë’¤ ê³µë°± ì—¬ë¶€ ê²€ì‚¬
                if re.search(r"[.!?]\s+[ê°€-í£A-Za-z]", original):
                    # ì´ê±´ ì •ìƒ êµ¬ì¡° â†’ ë²„ë¦°ë‹¤
                    continue
        fixed.append(line)
    return "\n".join(fixed)


def drop_false_korean_period_errors(report: str) -> str:
    """
    í•œêµ­ì–´ ë¦¬í¬íŠ¸ì—ì„œ, 'ì›ë¬¸' ë¶€ë¶„ì— ì´ë¯¸ ì¢…ê²°ë¶€í˜¸ê°€ ìˆëŠ”ë°
    'ë§ˆì¹¨í‘œê°€ ì—†ìŠµë‹ˆë‹¤' ë¥˜ë¡œ ì˜ëª» ë³´ê³ í•œ ì¤„ì„ ì œê±°í•œë‹¤.
    """
    if not report:
        return ""

    cleaned_lines = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':", re.UNICODE)
    bad_phrases = [
        "ë§ˆì¹¨í‘œê°€ ì—†ìŠµë‹ˆë‹¤",
        "ë§ˆì¹¨í‘œê°€ ë¹ ì ¸",
        "ë§ˆì¹¨í‘œê°€ í•„ìš”",
        "ë§ˆì¹¨í‘œë¥¼ ì°ì–´ì•¼",
        "ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œê°€ ì—†",
    ]

    for line in report.splitlines():
        s = line.strip()
        if not s:
            continue

        # ë§ˆì¹¨í‘œ ê´€ë ¨ ë©˜íŠ¸ê°€ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ í†µê³¼
        if not any(p in s for p in bad_phrases):
            cleaned_lines.append(s)
            continue

        m = pattern.match(s)
        if not m:
            cleaned_lines.append(s)
            continue

        original = m.group(1).rstrip()
        if not original:
            cleaned_lines.append(s)
            continue

        last = original[-1]
        ok = False
        if last in ".?!":
            ok = True
        elif len(original) >= 2 and last in ['"', "'", "â€", "â€™", "ã€", "ã€", "ã€‹", "ã€‰", ")", "]"] and original[-2] in ".?!":
            ok = True

        # ì´ë¯¸ ì¢…ê²°ë¶€í˜¸ê°€ ìˆìœ¼ë©´ â†’ ì´ ì¤„ì€ ê°€ì§œ ì˜¤ë¥˜ë¡œ ë³´ê³  ì œê±°
        if ok:
            continue
        else:
            cleaned_lines.append(s)

    return "\n".join(cleaned_lines)


def drop_lines_not_in_source(source_text: str, report: str) -> str:
    """
    report ì•ˆ '- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ':' íŒ¨í„´ì—ì„œ 'ì›ë¬¸'ì´
    1) ì‹¤ì œ source_textì— ì™„ì „ ë™ì¼í•˜ê²Œ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ìœ ì§€
    2) ë„ì–´ì“°ê¸° normalize í›„ì—ë„ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì œê±°
    3) ë¶€ë¶„ ë¬¸ìì—´ë§Œ ì¼ì¹˜í•  ê²½ìš°ë„ ì œê±°
    """
    if not report:
        return ""

    cleaned = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':", re.UNICODE)

    # normalize
    normalized_src = (
        source_text.replace(" ", "")
        .replace("\n", "")
        .replace("\u200b", "")
        .strip()
    )

    for line in report.splitlines():
        s = line.strip()
        if not s:
            continue

        m = pattern.match(s)
        if not m:
            cleaned.append(s)
            continue

        original = m.group(1)

        # ì™„ì „ ë™ì¼ ë§¤ì¹­ë§Œ í—ˆìš©
        if original in source_text:
            cleaned.append(s)
            continue

        # ë„ì–´ì“°ê¸° ì œê±° í›„ ë¹„êµ
        if original.replace(" ", "") in normalized_src:
            cleaned.append(s)
            continue

        # ê·¸ ì™¸ëŠ” drop
        continue

    return "\n".join(cleaned)


def drop_escape_false_positives(report: str) -> str:
    """
    \"\\\"\", \"\\'\", '\"/\"' ë“± escape/í¬ë§·íŒ… ì „ìš© í† í° ë•Œë¬¸ì—
    ë°œìƒí•˜ëŠ” ì˜ëª»ëœ ë”°ì˜´í‘œ/ë¬¸ì¥ë¶€í˜¸ ì˜¤ë¥˜ë¥¼ ì œê±°.
    """
    if not report:
        return report

    false_patterns = [
        r'\\\"',   # \"
        r'\\\'',   # \'
        r'\"/\"',
        r'\"',
        r'/\"',
        r'\"/',
    ]

    cleaned = []
    for line in report.splitlines():
        for p in false_patterns:
            if re.search(p, line):
                # escape ë¬¸ìì—´ë¡œ ì¸í•œ ì˜¤íŒ â†’ ì œê±°
                break
        else:
            cleaned.append(line)

    return "\n".join(cleaned)


def ensure_final_punctuation_error(text: str, report: str) -> str:
    """
    ë¬¸ë‹¨ ë§ˆì§€ë§‰ ë¬¸ì¥ì˜ ëì— ì¢…ê²°ë¶€í˜¸(. ? !)ê°€ ì—†ìœ¼ë©´
    reportì— í•´ë‹¹ ì˜¤ë¥˜ë¥¼ ê°•ì œë¡œ í•œ ì¤„ ì¶”ê°€í•œë‹¤. (í•œêµ­ì–´ ìª½ì—ì„œ ì£¼ë¡œ ì‚¬ìš©)
    """
    if not text or not text.strip():
        return report or ""

    s = text.rstrip()
    if not s:
        return report or ""

    last = s[-1]

    end_ok = False
    if last in ".?!":
        end_ok = True
    elif last in ['"', "'", "â€", "â€™", "ã€", "ã€", "ã€‹", "ã€‰", ")", "]"] and len(s) >= 2 and s[-2] in ".?!":
        end_ok = True

    if end_ok:
        return report or ""

    # ì´ë¯¸ ë¹„ìŠ·í•œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
    if report and ("ë§ˆì¹¨í‘œ" in report or "ë¬¸ì¥ë¶€í˜¸" in report):
        return report

    line = "- ë¬¸ë‹¨ ë§ˆì§€ë§‰ ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œ(ë˜ëŠ” ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ)ê°€ ë¹ ì ¸ ìˆìœ¼ë¯€ë¡œ ì ì ˆí•œ ë¬¸ì¥ë¶€í˜¸ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤."
    if report:
        return report.rstrip() + "\n" + line
    else:
        return line


def ensure_sentence_end_punctuation(text: str, report: str) -> str:
    """
    ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ ì¢…ê²°ë¶€í˜¸(. ? !) ì—†ëŠ” ë¬¸ì¥ì´ ìˆìœ¼ë©´
    í•œ ì¤„ë¡œ ìš”ì•½í•´ì„œ ë³´ê³ .
    """
    if not text or not text.strip():
        return report or ""

    # ê¸°ë³¸ ë¬¸ì¥ ë¶„ë¦¬
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())

    missing = []
    for i, s in enumerate(sentences):
        s = s.strip()
        if not s:
            continue

        ok = (
            s[-1] in ".?!"
            or (len(s) >= 2 and s[-1] in ['"', "'", "â€", "â€™", "ã€", "ã€", "ã€‹", "ã€‰", ")", "]"] and s[-2] in ".?!")
        )

        # ì¢…ê²°ë¶€í˜¸ ì—†ëŠ” ë¬¸ì¥ ìˆ˜ì§‘
        if not ok:
            missing.append(s)

    if not missing:
        return report or ""

    line = "- ë¬¸ì¥ ëì— ì¢…ê²°ë¶€í˜¸(., ?, !)ê°€ ëˆ„ë½ëœ ë¬¸ì¥ì´ ìˆìŠµë‹ˆë‹¤."

    if report:
        return report.rstrip() + "\n" + line
    else:
        return line


def clean_self_equal_corrections(report: str) -> str:
    """
    '- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ...' í˜•ì‹ì—ì„œ
    ì›ë¬¸ê³¼ ìˆ˜ì •ì•ˆì´ ì™„ì „íˆ ê°™ì€ ì¤„ì€ ì œê±°í•œë‹¤.
    (ì£¼ë¡œ ì˜ì–´ ìª½ content_typo_reportì— ì‚¬ìš©)
    """
    if not report:
        return ""

    cleaned_lines = []
    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':", re.UNICODE)

    for line in report.splitlines():
        line_stripped = line.strip()
        if not line_stripped:
            continue

        m = pattern.match(line_stripped)
        if not m:
            cleaned_lines.append(line_stripped)
            continue

        orig = m.group(1).strip()
        fixed = m.group(2).strip()

        if orig == fixed:
            # 'ì›ë¬¸' == 'ìˆ˜ì •ì•ˆ'ì¸ ë¼ì¸ì€ ì˜ë¯¸ ì—†ìœ¼ë¯€ë¡œ ì œê±°
            continue

        cleaned_lines.append(line_stripped)

    return "\n".join(cleaned_lines)


def drop_false_period_errors(english_text: str, report: str) -> str:
    """
    ì˜ì–´ ì›ë¬¸ ëì— ì‹¤ì œë¡œ . ? ! ì´ ìˆìœ¼ë©´
    ë¦¬í¬íŠ¸ì—ì„œ 'ë§ˆì¹¨í‘œ ì—†ìŒ'ë¥˜ ë¬¸ì¥ì„ ì œê±°.
    (ê±°ì§“ ì–‘ì„± ì¤„ì´ê¸°ìš©)
    """
    if not report:
        return ""

    stripped = (english_text or "").rstrip()
    last_char = stripped[-1] if stripped else ""

    if last_char in [".", "?", "!"]:
        bad_phrases = [
            "ë§ˆì¹¨í‘œê°€ ì—†ìŠµë‹ˆë‹¤",
            "ë§ˆì¹¨í‘œê°€ ë¹ ì ¸",
            "ë§ˆì¹¨í‘œê°€ í•„ìš”",
            "ë§ˆì¹¨í‘œë¥¼ ì°ì–´ì•¼",
        ]
        cleaned_lines = []
        for line in report.splitlines():
            if any(p in line for p in bad_phrases):
                continue
            cleaned_lines.append(line.strip())
        return "\n".join(cleaned_lines)

    return report


def split_report_by_source(report: str, plain_text: str, md_text: str) -> tuple[str, str]:
    """
    í•˜ë‚˜ì˜ ë¦¬í¬íŠ¸ë¥¼ 'ì›ë¬¸ì´ plainì—ì„œ ì˜¨ ê²ƒ' / 'ì›ë¬¸ì´ markdownì—ì„œ ì˜¨ ê²ƒ'ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.
    - "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ..." íŒ¨í„´ ê¸°ì¤€ìœ¼ë¡œ 'ì›ë¬¸'ì„ ë³´ê³  ì†Œì†ì„ ê²°ì •
    - ì›ë¬¸ì´ plainì—ë„ ìˆê³  mdì—ë„ ìˆìœ¼ë©´, ìš°ì„  plain ìª½ìœ¼ë¡œ ë³´ë‚¸ë‹¤.
    """
    if not report:
        return "", ""

    plain_lines: List[str] = []
    md_lines: List[str] = []

    pattern = re.compile(r"^- '(.+?)' â†’ '(.+?)':.*", re.UNICODE)

    for line in report.splitlines():
        s = line.strip()
        if not s:
            continue

        m = pattern.match(s)
        if not m:
            # íŒ¨í„´ì´ ì•„ë‹ˆë©´ ì¼ë‹¨ plain ìª½ì— ë„£ì–´ë‘”ë‹¤
            plain_lines.append(s)
            continue

        original = m.group(1)

        in_plain = original in (plain_text or "")
        in_md = original in (md_text or "")

        if in_plain and not in_md:
            plain_lines.append(s)
        elif in_md and not in_plain:
            md_lines.append(s)
        else:
            # ë‘˜ ë‹¤ í¬í•¨ë˜ê±°ë‚˜ ë‘˜ ë‹¤ ì•ˆ í¬í•¨ë˜ë©´ ìš°ì„  plainìœ¼ë¡œ
            plain_lines.append(s)

    return "\n".join(plain_lines), "\n".join(md_lines)


# ---------------------------------------------------
# 3-A. í•œêµ­ì–´ ë‹¨ì–´ ë‚´ë¶€ ë¶„ë¦¬(í˜•íƒœì†Œ ë¶„ë¦¬) í›„ì²˜ë¦¬ ì „ìš© ìœ í‹¸
# ---------------------------------------------------

# âœ… ë‹¨ì–´ ë‚´ë¶€ ë¶„ë¦¬ ì˜¤ë¥˜ ì˜ˆì™¸(ì •ìƒ í‘œí˜„) í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸
INTERNAL_SPLIT_WHITELIST: set[str] = {
    "í•  ìˆ˜",
    "ìˆ˜ ìˆ",
    "í•  ê²ƒ",
    "ê²ƒ ì´",
    "ìˆ ì„",
    "í•  ë¿",
    "ì¤‘ìš” í•œ",
    "ê°™ ì€ ì ",
    "ë‹¤ë¥¸ ì ",
    # í•„ìš”í•  ë•Œ ì ì  ì¶”ê°€í•´ì„œ ì‚¬ìš©
}


def find_korean_internal_split_candidates(text: str) -> list[tuple[str, str]]:
    """
    'ëœ ë‹¤', 'ë¬» ëŠ”' ê°™ì€ **ë‹¨ì–´ ë‚´ë¶€ ë¶„ë¦¬**ë§Œ í›„ë³´ë¡œ ì¡ëŠ”ë‹¤.
    - ì•/ë’¤ ê°ê° 1~2ê¸€ìì§œë¦¬ í•œê¸€ + ê³µë°± + 1~2ê¸€ìì§œë¦¬ í•œê¸€
    - INTERNAL_SPLIT_WHITELISTì— ìˆëŠ” ì •ìƒ í‘œí˜„ì€ ì œì™¸
    """
    if not text:
        return []

    candidates: list[tuple[str, str]] = []

    # ([ê°€-í£]{1,2}) + ê³µë°± + ([ê°€-í£]{1,2}) íŒ¨í„´ë§Œ ì¡ìŒ
    pattern = re.compile(r"([ê°€-í£]{1,2})\s+([ê°€-í£]{1,2})")

    for m in pattern.finditer(text):
        left = m.group(1)
        right = m.group(2)
        span_text = f"{left} {right}"

        # 1) whitelistì— ìˆìœ¼ë©´ ì •ìƒ ë„ì–´ì“°ê¸° â†’ ê±´ë„ˆë›°ê¸°
        if span_text in INTERNAL_SPLIT_WHITELIST:
            continue

        fixed = left + right
        candidates.append((span_text, fixed))

    return candidates


def build_internal_split_report(text: str) -> str:
    """
    find_korean_internal_split_candidates() ê²°ê³¼ë¥¼
    ë¦¬í¬íŠ¸ í˜•ì‹ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜í•œë‹¤.
    """
    errors = find_korean_internal_split_candidates(text)
    if not errors:
        return ""

    lines: list[str] = []
    for orig, fixed in errors:
        lines.append(
            f"- '{orig}' â†’ '{fixed}': '{orig}'ëŠ” ë‹¨ì–´ ë‚´ë¶€ ê³µë°±ì´ ì˜ëª» ë“¤ì–´ê°„ í˜•íƒœì†Œ ë¶„ë¦¬ ì˜¤ë¥˜ì´ë©° "
            f"'{fixed}'ë¡œ ë¶™ì—¬ ì¨ì•¼ í•©ë‹ˆë‹¤."
        )
    return "\n".join(lines)


# ---------------------------------------------------
# 3. í”„ë¡¬í”„íŠ¸ ì •ì˜ (ì˜ì–´ / í•œêµ­ì–´ ë¶„ë¦¬)
# ---------------------------------------------------

def create_english_review_prompt(text: str) -> str:
    """
    ì‹œíŠ¸ì˜ content(ì˜ì–´ ì›ë¬¸)ì— ëŒ€í•´ ê²€ìˆ˜í•˜ëŠ” í”„ë¡¬í”„íŠ¸.
    - ìŠ¤í ë§ / split-word / AIâ†”Al / ëŒ€ë¬¸ì / ê¸°ë³¸ ë¬¸ì¥ ë¶€í˜¸
    - ê²°ê³¼ëŠ” content_typo_report(í•œêµ­ì–´ ì„¤ëª…)ì—ë§Œ ìŒ“ì´ê²Œ ìœ ë„
    """
    return f"""
You are a machine-like **English text proofreader**.
Your ONLY job is to detect **objective, verifiable errors** in the following English text.
You MUST NOT suggest stylistic changes, paraphrasing, natural-sounding alternatives, tone changes, or meaning changes.

Your response MUST be a single valid JSON object with keys:
- "suspicion_score": integer 1~5
- "content_typo_report": string (Korean ì„¤ëª…)
- "translated_typo_report": ""   â† í•­ìƒ ë¹ˆ ë¬¸ìì—´
- "markdown_report": ""          â† í•­ìƒ ë¹ˆ ë¬¸ìì—´

All explanations in content_typo_report MUST be written in **Korean**.

If there are no errors:
- suspicion_score = 1
- all reports = ""

------------------------------------------------------------
# IMPORTANT ANTI-HALLUCINATION RULE
------------------------------------------------------------
- In the pattern "- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ...", the 'ì›ë¬¸' part MUST be a substring
  that actually appears in the input text `plain_english`.
- "ì›ë¬¸" MUST always be copied from `plain_english` exactly as it appears.
- You MUST NOT invent new tokens or reuse example tokens that do not literally
  appear in the input text.

------------------------------------------------------------
# 1. RULES FOR ENGLISH OBJECTIVE ERRORS
------------------------------------------------------------

## (A) Split-Word Errors (í•­ìƒ ì˜¤íƒ€ë¡œ ì·¨ê¸‰ â€” ë§¤ìš° ì¤‘ìš”)
If an English word appears with an incorrect internal space,
AND removing the space yields a valid English word,
you MUST treat it as a spelling error.

ALWAYS flag patterns like:
- "wi th"  â†’ "with"
- "dea th" â†’ "death"
- "o f"    â†’ "of"
- "amo ng" â†’ "among"
- "cont inents" â†’ "continents"

Report format (Korean):
"- 'wi th' â†’ 'with': 'wi th'ëŠ” ë‹¨ì–´ ë‚´ë¶€ ê³µë°±ì´ ì˜ëª»ëœ ì˜¤íƒ€ì´ë©° 'with'ë¡œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤."

## (B) Normal English spelling mistakes (MUST detect)
Any token similar to a valid English word (1â€“2 letters swapped/missing) MUST be flagged.

Examples (patterns, not exhaustive):
1. recieve â†’ receive
2. enviroment â†’ environment
3. understaning â†’ understanding
4. langauge â†’ language
5. problme â†’ problem
6. definately â†’ definitely
7. seperated â†’ separated
8. occured â†’ occurred
9. adress â†’ address
10. wierd â†’ weird
11. becuase â†’ because
12. comming â†’ coming
13. teh â†’ the

## (C) AI ë¬¸ë§¥ì—ì„œ "Al" â†’ "AI" (í•­ìƒ ì¡ê¸°)
If the surrounding sentence mentions:
model / system / tool / chatbot / LLM / agent / dataset / training / inference
then â€œAlâ€ (A+ì†Œë¬¸ì l) MUST be interpreted as a typo for â€œAIâ€.

Examples:
- "Al model" â†’ "AI model"
- "Al system" â†’ "AI system"

## (D) Capitalization Errors
You MUST flag:
- Sentence starting with lowercase
- Pronoun â€œIâ€ written as â€œiâ€
- Proper nouns not capitalized (london â†’ London)

## (E) Duplicate / spacing errors
- "the the"
- "re turn" â†’ "return"
- "mod el" â†’ "model"

## (F) STRICT punctuation rule â€” avoid false positives
You MUST NOT report a punctuation error if the text already ends with ANY of:
- ".", "?", "!"
- ".*\"", ".*!\"", ".*?\""
- ".*â€™", ".*!â€™", ".*?â€™"

ONLY report a punctuation error if:
- the sentence has NO ending punctuation at all, OR
- a closing quotation mark is missing, OR
- punctuation is clearly malformed (e.g. ",.", ".,", "..", "!!", "??" in a wrong place)

------------------------------------------------------------
# 2. OUTPUT FORMAT
------------------------------------------------------------
You MUST output EXACTLY ONE JSON object (no extra text, no markdown).

Each error line example (in Korean):

"- 'understaning' â†’ 'understanding': 'understaning'ì€ ì² ì ì˜¤íƒ€ì´ë©° 'understanding'ìœ¼ë¡œ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤."

If there is NO objective error at all:
- "suspicion_score": 1
- "content_typo_report": ""
- "translated_typo_report": ""
- "markdown_report": ""

------------------------------------------------------------
# 3. TEXT TO REVIEW
plain_english: \"\"\"{text}\"\"\"
"""


def create_korean_review_prompt(text: str) -> str:
    """
    ì‹œíŠ¸ì˜ content_translated(í•œêµ­ì–´ ë²ˆì—­)ì— ëŒ€í•´ ê²€ìˆ˜í•˜ëŠ” í”„ë¡¬í”„íŠ¸.
    - ì˜¤íƒˆì / ì¡°ì‚¬Â·ì–´ë¯¸ / ë„ì–´ì“°ê¸° / í˜•íƒœì†Œ ë¶„ë¦¬ / ë°˜ë³µ / ë¬¸ì¥ë¶€í˜¸
    - ê²°ê³¼ëŠ” translated_typo_reportì—ë§Œ ìŒ“ì´ê²Œ ìœ ë„
    """
    return f"""
ë‹¹ì‹ ì€ ê¸°ê³„ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” **Korean text proofreader**ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ìœ ì¼í•œ ì„ë¬´ëŠ” ì•„ë˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ **ê°ê´€ì ì´ê³  í™•ì¸ ê°€ëŠ¥í•œ ì˜¤ë¥˜ë§Œ** ì°¾ì•„ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤.

============================================================
ğŸš¨ ê°€ì¥ ì¤‘ìš”í•œ ê·œì¹™ (ì›ë¬¸ ë³´ì¡´ â€” ì ˆëŒ€ ìœ„ë°˜ ê¸ˆì§€)
============================================================
ì•„ë˜ ì§ˆë¬¸ ì¤‘ í•˜ë‚˜ë¼ë„ â€œì˜ˆâ€ë¼ë©´, ê·¸ ìˆ˜ì •ì€ **ë³´ê³ í•˜ì§€ ë§ê³  ì™„ì „íˆ ë¬´ì‹œ**í•´ì•¼ í•©ë‹ˆë‹¤.

1) ìˆ˜ì •í•˜ë ¤ëŠ” ë¶€ë¶„ì´ plain_koreanì— **ê·¸ëŒ€ë¡œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ê°€?**
2) ë‹¨ì–´ **ìˆœì„œë¥¼ ë³€ê²½**í•´ì•¼ í•˜ëŠ”ê°€?
3) ì˜ë¯¸ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆëŠ” ìˆ˜ì •ì¸ê°€?
4) ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ **ì¶”ê°€í•´ì•¼ë§Œ** ìˆ˜ì •ì´ ê°€ëŠ¥í•œê°€?
5) ìì—°ìŠ¤ëŸ½ê²Œ ë“¤ë¦¬ë„ë¡ **ë‹¤ë“¬ëŠ” ê²ƒ**ì²˜ëŸ¼ ë³´ì´ëŠ”ê°€?
6) ë¬¸ì¥ì„ ì‚¬ì‹¤ìƒ **ë‹¤ì‹œ ì“°ëŠ” ê²ƒì²˜ëŸ¼** ë³´ì´ëŠ”ê°€?

â†’ í•˜ë‚˜ë¼ë„ â€œì˜ˆâ€ë¼ë©´, í•´ë‹¹ ì˜¤ë¥˜ëŠ” **ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.**

============================================================
ğŸš« Hallucination ë°©ì§€ ê·œì¹™
============================================================
âŒ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´/êµ¬ì ˆ ìƒì„± ê¸ˆì§€  
âŒ í”„ë¡¬í”„íŠ¸ ì„¤ëª…ë¶€ì— ìˆëŠ” ë‹¨ì–´ë¥¼ â€˜ì›ë¬¸â€™ìœ¼ë¡œ ì¬ì‚¬ìš© ê¸ˆì§€  
âŒ ì›ë¬¸ì˜ ë¬¸ì¥ êµ¬ì¡°Â·ì˜ë„Â·í†¤Â·ì–´ìˆœ ë³€ê²½ ê¸ˆì§€  

'- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ...' í˜•ì‹ì˜ 'ì›ë¬¸'ì€  
ë°˜ë“œì‹œ plain_korean ì•ˆì— **ë¬¸ì ë‹¨ìœ„ë¡œ ë™ì¼í•˜ê²Œ ì¡´ì¬**í•´ì•¼ í•©ë‹ˆë‹¤.

============================================================
ğŸ“Œ ì•ˆì „ ì˜ˆì‹œ (ë”ë¯¸ í† í° â€” ì¶œë ¥ ê¸ˆì§€)
============================================================
ì•„ë˜ ì˜ˆì‹œëŠ” í—ˆìš©ë˜ëŠ” â€œìˆ˜ì • í¬ê¸°ì˜ ë²”ìœ„â€ë§Œ ì„¤ëª…í•˜ê¸° ìœ„í•œ ê²ƒì´ë©°  
AAA/BBB/CCC ë“±ì€ ì‹¤ì œ í…ìŠ¤íŠ¸ì— ì—†ëŠ” **ë”ë¯¸ í† í°**ì…ë‹ˆë‹¤.  
ì¶œë ¥ì— ë“±ì¥í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.

- 'AAAë¥¼ë¥¼' â†’ 'AAAë¥¼' : ì¡°ì‚¬ ì¤‘ë³µ ìˆ˜ì •(1~2ê¸€ì)
- 'BBB ì„' â†’ 'BBBì„' : ê³µë°±/ì¡°ì‚¬ ì˜¤ìš©(ê·¹ì†Œìˆ˜ ë³€ê²½)
- 'CCC í•œ ë‹¤' â†’ 'CCCí•œë‹¤' : ë‹¨ì–´ ë‚´ë¶€ ê³µë°±(í˜•íƒœì†Œ ë¶„ë¦¬ ì˜¤ë¥˜)
- 'DDDë‹¤ë‹¤ë‹¤' â†’ 'DDDë‹¤' : ë°˜ë³µ ì˜¤íƒ€ ì •ë¦¬

â€» ìœ„ ì˜ˆì‹œëŠ” ë‹¨ìˆœ ì„¤ëª…ìš©ì´ë©° ì‹¤ì œ ì¶œë ¥ì— í¬í•¨ë˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.

============================================================
# 1. í•œêµ­ì–´ì—ì„œ ë°˜ë“œì‹œ ì¡ì•„ì•¼ í•˜ëŠ” ê°ê´€ì  ì˜¤ë¥˜
============================================================

(A) ì˜¤íƒˆì / ì² ì ì˜¤ë¥˜  
(B) ì¡°ì‚¬Â·ì–´ë¯¸ ì˜¤ë¥˜  
(C) ë‹¨ì–´ ë‚´ë¶€ ë¶ˆí•„ìš”í•œ ê³µë°±  
(D) ë°˜ë³µ ì˜¤íƒ€  
(E) ëª…ë°±í•œ ë„ì–´ì“°ê¸° ì˜¤ë¥˜  
(F) ë¬¸ì¥ë¶€í˜¸ ì˜¤ë¥˜  
   - ë¬¸ì¥ ëì— ì¢…ê²°ë¶€í˜¸ ì—†ìŒ  
   - ë”°ì˜´í‘œ ì§ ë¶ˆì¼ì¹˜  
   - ëª…ë°±íˆ ì˜ëª»ëœ ì‰¼í‘œ  
   - ë¬¸ì¥ ì¤‘ê°„ì˜ ë¶ˆí•„ìš”í•œ ë§ˆì¹¨í‘œ/ì‰¼í‘œ  

[G] ë¬¸ì¥ë¶€í˜¸ ë’¤ ê³µë°± ê·œì¹™ (ì¤‘ìš”)
- ë¬¸ì¥ ëì— ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œê°€ ìˆê³ , ê·¸ ë’¤ì—ì„œ ìƒˆë¡œìš´ ë¬¸ì¥ì´ ì‹œì‘ë  ê²½ìš°,
  ë¬¸ì¥ë¶€í˜¸ ë’¤ì˜ ê³µë°±ì€ **ì •ìƒì´ë©° ì˜¤íƒ€ê°€ ì•„ë‹ˆë‹¤.**
- ê·¸ëŸ¬ë¯€ë¡œ "í˜ë¦°ë‹¤. í…”ë ˆë¹„ì „"ì²˜ëŸ¼ 
  ì¢…ê²°ë¶€í˜¸ + ê³µë°± + ìƒˆë¡œìš´ ë¬¸ì¥ì´ ì‹œì‘ë˜ëŠ” êµ¬ì¡°ëŠ” ì ˆëŒ€ë¡œ ì˜¤ë¥˜ë¡œ íŒë‹¨í•˜ì§€ ì•ŠëŠ”ë‹¤.
- ë‹¨ì–´ ë‚´ë¶€ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±(ì˜ˆ: 'í˜ ë¦°ë‹¤', 'í•œë‹¤ ë‹¤')ë§Œ ì˜¤ë¥˜ë¡œ ì¸ì •í•œë‹¤.

ë‹¤ìŒê³¼ ê°™ì€ íŒ¨í„´ì€ ë¬¸ë²•ì  ì˜¤ë¥˜ê°€ ì•„ë‹ˆë¯€ë¡œ ì ˆëŒ€ë¡œ ì˜¤ë¥˜ë¡œ ë³´ê³ í•˜ì§€ ì•ŠëŠ”ë‹¤.
- \"  â† JSON/markdownì—ì„œ ì“°ëŠ” escape ë¬¸ì
- \'  
- \"\"  
- /"  
- "/  
- */"  
- '"'
- markdown code block ê¸°ìˆ ì—ì„œ ì‚¬ìš©ë˜ëŠ” `\"`, `\'`, `\(` ë“±

ì´ë“¤ì€ ë‹¨ìˆœí•œ escape ë˜ëŠ” ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ…ì¼ ë¿ì´ë©°,
ë”°ì˜´í‘œ ì§ ë¶ˆì¼ì¹˜ë‚˜ ë¬¸ì¥ë¶€í˜¸ ì˜¤ë¥˜ë¡œ ê°„ì£¼í•´ì„œëŠ” ì•ˆ ëœë‹¤.

íŠ¹íˆ plain_korean ì „ì²´ì˜ **ë§ˆì§€ë§‰ ë¬¸ì¥ ëì— ì¢…ê²°ë¶€í˜¸ê°€ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ ì˜¤ë¥˜ë¡œ ë³´ê³ í•´ì•¼ í•©ë‹ˆë‹¤.**

============================================================
# 2. Output Format (JSON Only)
============================================================
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ bullet ë‹¨ìœ„ ì˜¤ë¥˜ë¥¼ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤:

"- 'ì›ë¬¸' â†’ 'ìˆ˜ì •ì•ˆ': ì˜¤ë¥˜ ì„¤ëª…"

ì˜¤ë¥˜ê°€ ì—†ë‹¤ë©´:
- suspicion_score = 1
- content_typo_report = ""
- translated_typo_report = ""
- markdown_report = ""

============================================================
# 3. TEXT TO REVIEW
============================================================
plain_korean: "/"/"{text}"/"/"
"""


# ---------------------------------------------------
# 3-1. ê³µí†µ: Gemini í˜¸ì¶œ / ê²°ê³¼ ì •ì œ
# ---------------------------------------------------

def analyze_text_with_gemini(prompt: str, max_retries: int = 5) -> dict:
    """Geminië¥¼ JSON ëª¨ë“œë¡œ í˜¸ì¶œ + ì¬ì‹œë„ ë¡œì§"""
    last_error: Exception | None = None

    for attempt in range(max_retries):
        try:
            generation_config = {
                "response_mime_type": "application/json",
                "temperature": 0.0,
            }
            response = model.generate_content(
                prompt,
                generation_config=generation_config,
            )
            return json.loads(response.text)

        except Exception as e:
            last_error = e
            wait_time = 5 * (attempt + 1)
            print(f"Gemini í˜¸ì¶œ ì˜¤ë¥˜ (ì‹œë„ {attempt+1}/{max_retries}): {e}")

            if attempt < max_retries - 1:
                print(f"â†’ {wait_time}ì´ˆ í›„ ì¬ì‹œë„")
                time.sleep(wait_time)

    print("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼.")
    return {
        "suspicion_score": 5,
        "content_typo_report": f"API í˜¸ì¶œ ì‹¤íŒ¨: {str(last_error)}",
        "translated_typo_report": "",
        "markdown_report": "",
    }


def validate_and_clean_analysis(result: dict) -> dict:
    """
    ëª¨ë¸ ì‘ë‹µì˜ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ë³´ì • + ìŠ¤íƒ€ì¼/ë¬¸ì²´ì„± ë©˜íŠ¸ í•„í„°ë§.
    """
    # 0) Geminiê°€ ì—ëŸ¬ JSONì„ ëŒë ¤ì¤€ ê²½ìš° ì²˜ë¦¬
    if isinstance(result, dict) and "ERROR" in result:
        err_obj = result.get("ERROR") or {}
        if isinstance(err_obj, dict):
            msg = err_obj.get("message") or str(err_obj)
        else:
            msg = str(err_obj)

        return {
            "suspicion_score": 5,
            "content_typo_report": f"Gemini API ë‚´ë¶€ ì˜¤ë¥˜: {msg}",
            "translated_typo_report": "",
            "markdown_report": "",
        }

    # 1) ì•„ì˜ˆ dictê°€ ì•„ë‹ ë•Œ
    if not isinstance(result, dict):
        return {
            "suspicion_score": 5,
            "content_typo_report": "AI ì‘ë‹µì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹˜",
            "translated_typo_report": "",
            "markdown_report": "",
        }

    score = result.get("suspicion_score")
    reports = {
        "content_typo_report": result.get("content_typo_report", "") or "",
        "translated_typo_report": result.get("translated_typo_report", "") or "",
        "markdown_report": result.get("markdown_report", "") or "",
    }

    # ìŠ¤íƒ€ì¼/ë¬¸ì²´ ì œì•ˆ ê¸ˆì§€ í‚¤ì›Œë“œ í•„í„°
    forbidden_keywords = [
        "ë¬¸ë§¥ìƒ",
        "ë¶€ì ì ˆ",
        "ì–´ìƒ‰",
        "ë” ìì—°ìŠ¤ëŸ½",
        "ë” ì ì ˆ",
        "ìˆ˜ì •í•˜ëŠ” ê²ƒì´ ì¢‹",
        "ì œì•ˆ",
        "ë°”ê¾¸ëŠ” ê²ƒ",
        "ì˜ë¯¸ë¥¼ ëª…í™•íˆ",
    ]
    for key, text in reports.items():
        if any(kw in text for kw in forbidden_keywords):
            reports[key] = ""

    # "ì˜¤ë¥˜ ì—†ìŒ"ë¥˜ ë©˜íŠ¸ ì œê±°
    forbidden_phrases = ["ì˜¤ë¥˜ ì—†ìŒ", "ì •ìƒ", "ë¬¸ì œ ì—†ìŒ", "ìˆ˜ì •í•  í•„ìš” ì—†ìŒ"]
    for key, text in reports.items():
        if any(ph in text for ph in forbidden_phrases):
            reports[key] = ""

    # ì˜ì–´ ë¦¬í¬íŠ¸ì— ëŒ€í•´ì„œ self equal ì •ë¦¬ (ì‹œíŠ¸ ì˜ì–´ ìª½ì—ì„œ ì‚¬ìš©)
    reports["content_typo_report"] = clean_self_equal_corrections(reports["content_typo_report"])

    # score ê¸°ë³¸ê°’ ë³´ì •
    try:
        score = int(score)
    except Exception:
        score = 1

    if score < 1:
        score = 1
    if score > 5:
        score = 5

    if not reports["content_typo_report"] and not reports["translated_typo_report"] and not reports["markdown_report"]:
        score = 1
    elif (reports["content_typo_report"] or reports["translated_typo_report"] or reports["markdown_report"]) and score == 1:
        score = 3

    return {
        "suspicion_score": score,
        "content_typo_report": reports["content_typo_report"],
        "translated_typo_report": reports["translated_typo_report"],
        "markdown_report": reports["markdown_report"],
    }


# ---------------------------------------------------
# 3-2. í•œ í–‰(ì˜ì–´+í•œêµ­ì–´)ì„ í†µí•© ê²€ìˆ˜í•˜ëŠ” í—¬í¼
# ---------------------------------------------------

def analyze_row_with_both_langs(row: Dict[str, Any]):
    """
    í•œ í–‰(row)ì— ëŒ€í•´:
      - content / content_markdown (ì˜ì–´)
      - content_translated / content_markdown_translated (í•œêµ­ì–´)
    ë¥¼ ëª¨ë‘ í•©ì³ì„œ í•œ ë²ˆì— ê²€ìˆ˜í•œë‹¤.
    """

    # 1) ì›ë³¸ í…ìŠ¤íŠ¸ë“¤ ê°€ì ¸ì˜¤ê¸°
    en_plain = (row.get(ORIGINAL_TEXT_COL) or "").strip()
    en_md = (row.get(ORIGINAL_MD_COL) or "").strip()
    ko_plain = (row.get(TRANSLATION_TEXT_COL) or "").strip()
    ko_md = (row.get(TRANSLATION_MD_COL) or "").strip()

    # 2) ì‹¤ì œë¡œ ëª¨ë¸ì— ë³´ë‚¼ í†µí•© í…ìŠ¤íŠ¸ (ë¹ˆ ê±´ ì œì™¸í•˜ê³  ì¤„ë°”ê¿ˆìœ¼ë¡œ ì´ì–´ ë¶™ì´ê¸°)
    en_text = "\n".join(t for t in [en_plain, en_md] if t)
    ko_text = "\n".join(t for t in [ko_plain, ko_md] if t)

    raw_en = final_en = None
    raw_ko = final_ko = None

    # --- ì˜ì–´ ìª½ ---
    if en_text:
        prompt_en = create_english_review_prompt(en_text)
        raw_en = analyze_text_with_gemini(prompt_en)
        final_en = validate_and_clean_analysis(raw_en)

        filtered_en = drop_lines_not_in_source(
            en_text,  # âœ… í†µí•© í…ìŠ¤íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            final_en.get("content_typo_report", "") or "",
        )
        filtered_en = drop_false_period_errors(en_text, filtered_en)
        filtered_en = ensure_sentence_end_punctuation(en_text, filtered_en)
        final_en["content_typo_report"] = filtered_en
    else:
        final_en = {
            "suspicion_score": 1,
            "content_typo_report": "",
            "translated_typo_report": "",
            "markdown_report": "",
        }

    # --- í•œêµ­ì–´ ìª½ ---
    if ko_text:
        prompt_ko = create_korean_review_prompt(ko_text)
        raw_ko = analyze_text_with_gemini(prompt_ko)
        final_ko = validate_and_clean_analysis(raw_ko)

        filtered_ko = drop_lines_not_in_source(
            ko_text,  # âœ… í†µí•© í…ìŠ¤íŠ¸ ê¸°ì¤€
            final_ko.get("translated_typo_report", "") or "",
        )
        filtered_ko = drop_false_korean_period_errors(filtered_ko)
        filtered_ko = ensure_final_punctuation_error(ko_text, filtered_ko)
        filtered_ko = ensure_sentence_end_punctuation(ko_text, filtered_ko)
        filtered_ko = drop_escape_false_positives(filtered_ko)
        filtered_ko = dedup_korean_bullet_lines(filtered_ko)

        # ğŸ”¹ ì¶”ê°€: ë‹¨ì–´ ë‚´ë¶€ ë¶„ë¦¬(í˜•íƒœì†Œ ë¶„ë¦¬) ì „ìš© ê·œì¹™ ë¦¬í¬íŠ¸ ë§ë¶™ì´ê¸°
        internal_report = build_internal_split_report(ko_text)
        if internal_report:
            if filtered_ko:
                filtered_ko = filtered_ko.rstrip() + "\n" + internal_report
            else:
                filtered_ko = internal_report

        filtered_ko = drop_false_punctuation_space_errors(ko_text, filtered_ko)
        final_ko["translated_typo_report"] = filtered_ko
    else:
        final_ko = {
            "suspicion_score": 1,
            "content_typo_report": "",
            "translated_typo_report": "",
            "markdown_report": "",
        }

    # --- plain / markdown ê¸°ì¤€ìœ¼ë¡œ ë¦¬í¬íŠ¸ ë¶„ë¦¬ ---
    en_plain_report, en_md_report = split_report_by_source(
        final_en.get("content_typo_report", "") or "",
        en_plain,
        en_md,
    )
    ko_plain_report, ko_md_report = split_report_by_source(
        final_ko.get("translated_typo_report", "") or "",
        ko_plain,
        ko_md,
    )

    # plain ìª½ì€ ê¸°ì¡´ ì»¬ëŸ¼ì— ë‚¨ê¸°ê³ 
    final_en["content_typo_report"] = en_plain_report
    final_ko["translated_typo_report"] = ko_plain_report

    # markdownì—ì„œ ë‚˜ì˜¨ ì˜¤ë¥˜ëŠ” MARKDOWN_REPORTë¡œ ëª¨ìœ¼ê¸°
    markdown_report_parts: List[str] = []
    if en_md_report:
        markdown_report_parts.append(en_md_report)
    if ko_md_report:
        markdown_report_parts.append(ko_md_report)
    markdown_report = "\n".join(markdown_report_parts)

    # --- í†µí•© ìŠ¤ì½”ì–´ ---
    combined_final = {
        "suspicion_score": max(
            final_en.get("suspicion_score", 1),
            final_ko.get("suspicion_score", 1),
        ),
        "content_typo_report": final_en.get("content_typo_report", ""),
        "translated_typo_report": final_ko.get("translated_typo_report", ""),
        "markdown_report": markdown_report,
    }

    debug_bundle = {
        "english": {
            "text_plain": en_plain,
            "text_markdown": en_md,
            "text": en_text,  # ì‹¤ì œë¡œ ê²€ìˆ˜í•œ í†µí•© í…ìŠ¤íŠ¸
            "raw": raw_en,
            "final": final_en,
        },
        "korean": {
            "text_plain": ko_plain,
            "text_markdown": ko_md,
            "text": ko_text,
            "raw": raw_ko,
            "final": final_ko,
        },
    }

    return combined_final, debug_bundle


# ---------------------------------------------------
# 4. ê³µê°œ í•¨ìˆ˜: ì‹œíŠ¸ ì „ì²´ë¥¼ ëŒë¦¬ê³  ìš”ì•½ ë¦¬í„´
# ---------------------------------------------------

def run_sheet_review(
    spreadsheet_name: str,
    worksheet_name: str,
    collect_raw: bool = False,
    progress_callback=None,
) -> dict:
    """
    - ì£¼ì–´ì§„ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ / ì›Œí¬ì‹œíŠ¸ì—ì„œ
    - STATUS == '1. AIê²€ìˆ˜ìš”ì²­' ì¸ í–‰ë§Œ ê³¨ë¼ì„œ
    - SCORE / *_REPORT / STATUSë¥¼ ì±„ì›Œë„£ëŠ”ë‹¤.

    ë°˜í™˜ê°’: {
      "total_rows": ...,
      "target_rows": ...,
      "processed_rows": ...,
      "raw_results": [  # collect_raw=Trueì¼ ë•Œë§Œ
          {
            "sheet_row_index": int,
            "english": {"text", "raw", "final"},
            "korean": {"text", "raw", "final"},
            "combined_final": {...},
          },
          ...
      ]
    }
    """
    try:
        spreadsheet = gs_client.open(spreadsheet_name)
    except gspread.exceptions.SpreadsheetNotFound:
        raise ValueError(f"ìŠ¤í”„ë ˆë“œì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {spreadsheet_name}")

    try:
        worksheet = spreadsheet.worksheet(worksheet_name)
    except gspread.exceptions.WorksheetNotFound:
        raise ValueError(f"ì›Œí¬ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {worksheet_name}")

    all_data = worksheet.get_all_records()
    df = pd.DataFrame(all_data)
    df["sheet_row_index"] = df.index + 2  # 1í–‰ì€ í—¤ë”ë¼ì„œ +2

    targets = df[df[STATUS_COL] == "1. AIê²€ìˆ˜ìš”ì²­"].copy()
    if targets.empty:
        return {
            "total_rows": len(df),
            "target_rows": 0,
            "processed_rows": 0,
            "raw_results": [],
        }

    results: List[Dict[str, Any]] = []
    raw_results: List[Dict[str, Any]] = []

    total_targets = len(targets)

    for i, (_, row) in enumerate(targets.iterrows(), start=1):
        row_dict = row.to_dict()
        row_idx = row["sheet_row_index"]
        print(f"í–‰ {row_idx} ê²€ìˆ˜ ì¤‘... ({i}/{total_targets})")

        if progress_callback is not None:
            progress_callback(i, total_targets)

        # ğŸ”¹ ì˜ì–´ + í•œêµ­ì–´ í†µí•© ê²€ìˆ˜
        combined_final, debug_bundle = analyze_row_with_both_langs(row_dict)

        results.append(
            {
                "sheet_row_index": row_idx,
                SUSPICION_SCORE_COL: combined_final.get("suspicion_score"),
                CONTENT_TYPO_REPORT_COL: combined_final.get("content_typo_report"),
                TRANSLATED_COL: combined_final.get("translated_typo_report"),
                MARKDOWN_REPORT_COL: combined_final.get("markdown_report"),
                STATUS_COL: "2. AIê²€ìˆ˜ì™„ë£Œ",
            }
        )

        if collect_raw:
            raw_results.append(
                {
                    "sheet_row_index": row_idx,
                    **debug_bundle,
                    "combined_final": combined_final,
                }
            )

        time.sleep(0.5)  # API ê³¼ë‹¤ í˜¸ì¶œ ë°©ì§€ìš© (í•„ìš”ì‹œ ì¡°ì •)

    # === ì‹œíŠ¸ì— ê²°ê³¼ ë°˜ì˜ ===
    headers = worksheet.row_values(1)
    score_col_idx = headers.index(SUSPICION_SCORE_COL) + 1
    content_col_idx = headers.index(CONTENT_TYPO_REPORT_COL) + 1
    translated_col_idx = headers.index(TRANSLATED_COL) + 1
    markdown_col_idx = headers.index(MARKDOWN_REPORT_COL) + 1
    status_col_idx = headers.index(STATUS_COL) + 1

    def sanitize(v):
        return "" if v is None else str(v)

    update_cells = []
    for r in results:
        ridx = r["sheet_row_index"]
        update_cells.append(gspread.Cell(ridx, score_col_idx, sanitize(r[SUSPICION_SCORE_COL])))
        update_cells.append(gspread.Cell(ridx, content_col_idx, sanitize(r[CONTENT_TYPO_REPORT_COL])))
        update_cells.append(gspread.Cell(ridx, translated_col_idx, sanitize(r[TRANSLATED_COL])))
        update_cells.append(gspread.Cell(ridx, markdown_col_idx, sanitize(r[MARKDOWN_REPORT_COL])))
        update_cells.append(gspread.Cell(ridx, status_col_idx, sanitize(r[STATUS_COL])))

    if update_cells:
        worksheet.update_cells(update_cells)

    return {
        "total_rows": len(df),
        "target_rows": len(targets),
        "processed_rows": len(results),
        "raw_results": raw_results,
    }
